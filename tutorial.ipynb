{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 0.0: import dependancies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## import libraries for the analysis\n",
                "import os,sys,json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "##import modules from other folders\n",
                "current_working_directory = Path.cwd()\n",
                "parent_dir = current_working_directory.resolve().parents[0]\n",
                "sys.path.insert(0, str(parent_dir) + \"\\\\utilities\")\n",
                "from useful_tools import select_animals_gpt\n",
                "from data_cleaning import preprocess_fictrac_data\n",
                "sys.path.insert(0, str(parent_dir) + \"\\\\bonfic\")\n",
                "from analyse_stimulus_evoked_response import main"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 0.1: Load analysis methods in python dictionary form"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "json_file = \"./analysis_methods_dictionary.json\"\n",
                "with open(json_file, \"r\") as f:\n",
                "    analysis_methods = json.loads(f.read())\n",
                "    \n",
                "sheet_name=\"Zball\"\n",
                "Datasets=\"Z:/DATA/experiment_trackball_Optomotor\"\n",
                "thisDataset = f\"{Datasets}/{sheet_name}\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 0.2: check methods to use and whether some methods should be updated"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "analysis_methods"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 0.3: Load animal directory as a list"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## this cell searches for a folder with a specified experiment_name under the dataset path and list up all the csv file in that folder.\n",
                "## In this project, we usually have one csv file in that folder so there is no confusion\n",
                "dir_list = []\n",
                "file_type=\".dat\"\n",
                "for root, dirs, files in os.walk(thisDataset):\n",
                "    if analysis_methods.get(\"experiment_name\") in root.split(\n",
                "        os.path.sep\n",
                "    ):  ## add this condition to avoid data from other experiments\n",
                "        for folder in dirs:\n",
                "            if folder.startswith(\"session\"):\n",
                "                folder_path=os.path.join(root,folder)\n",
                "                if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
                "                    dir_list.append(folder_path.replace(\"\\\\\", \"/\"))\n",
                "\n",
                "\n",
                "print(f\"these directories are found {dir_list}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_session_folders(base_directory, file_type, paradigm_name):\n",
                "    session_folders = []\n",
                "\n",
                "    for root, dirs, files in os.walk(base_directory):\n",
                "        # Check if the target folder (e.g., 'apple') is in the root path and the paradigm name is in the root path\n",
                "        if paradigm_name in root.split(os.path.sep):\n",
                "            for folder in dirs:\n",
                "                # Check if the folder name starts with 'session'\n",
                "                if folder.startswith(\"session\"):\n",
                "                    folder_path = os.path.join(root, folder)\n",
                "                    # Check if the folder contains at least one file with the specified file type\n",
                "                    if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
                "                        session_folders.append(folder_path)\n",
                "\n",
                "    return session_folders\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "base_directory = thisDataset\n",
                "file_type = \".dat\"\n",
                "paradigm_name = analysis_methods.get(\"experiment_name\")\n",
                "\n",
                "session_folders = find_session_folders(base_directory, file_type, paradigm_name)\n",
                "\n",
                "print(f\"These directories are found: {session_folders}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 1.0: Create fictrac curated dataset based on the list of directories"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# because the import does not update the new version of python.\n",
                "# Need to restart kernel \n",
                "for this_dir in dir_list:\n",
                "    if \"database_curated.pickle\" in os.listdir(this_dir):\n",
                "        print(f\"curated fictrac data found in {this_dir}. Skip this file\")\n",
                "        continue\n",
                "    else:\n",
                "        print(f\"no curated fictrac data in {this_dir}. Create curated file\")\n",
                "        preprocess_fictrac_data(this_dir,analysis_methods)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 1.5: load particular animals into directory list for further analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the path to your Excel file\n",
                "dir_list = []\n",
                "file_type=\".pickle\"\n",
                "using_google_sheet=True\n",
                "sheet_name = \"VCCball\"\n",
                "experiment_name=analysis_methods.get(\"experiment_name\")\n",
                "if analysis_methods.get(\"load_experiment_condition_from_database\") == True:\n",
                "    if using_google_sheet==True:\n",
                "        database_id = \"1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8\"\n",
                "                #https://docs.google.com/spreadsheets/d/1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8/edit?usp=sharing\n",
                "        url = f\"https://docs.google.com/spreadsheets/d/{database_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
                "        #df = pd.read_excel(url, engine='openpyxl')## use this function if the file is not google sheet but uploaded excel file\n",
                "\n",
                "        df = pd.read_csv(url)\n",
                "    else:\n",
                "        excel_file_path = \"Z:/DATA/experiment_trackball_Optomotor/Locusts Management.xlsx\"\n",
                "        print(f\"using a database {excel_file_path} from the server but this file might be outdated\")\n",
                "        # Create a 'with' statement to open and read the Excel file\n",
                "        with pd.ExcelFile(excel_file_path) as xls:\n",
                "            # Read the Excel sheet into a DataFrame with the sheet name (folder name)\n",
                "            df = pd.read_excel(xls, sheet_name)\n",
                "        ##list up the conditions and answers as strings for input argument to select animal. One condition must pair with one answer\n",
                "    if analysis_methods.get(\"select_animals_by_condition\") == True:\n",
                "        animal_of_interest=select_animals_gpt(df,\"Experimenter\",\"NS\")\n",
                "        #print(animal_of_interest)\n",
                "    else:\n",
                "        animal_of_interest=df\n",
                "    ID_array=animal_of_interest[\"ID\"].values\n",
                "    dir_list = [\n",
                "    root.replace(\"\\\\\", \"/\")\n",
                "    for root, dirs, files in os.walk(thisDataset)\n",
                "    if any(ID in root for ID in ID_array)\n",
                "    and experiment_name in root.split(os.path.sep)\n",
                "    and any(name.endswith(file_type) for name in files)\n",
                "\n",
                "\n",
                "    \n",
                "]\n",
                "else:\n",
                "    ## this cell searches for a folder with a specified experiment_name under the dataset path and list up all the hdf5 file in that folder.\n",
                "    ## However,some changes need to be made once we do sleap or deeplabcut where there are more than one H5 file generated\n",
                "    for root, dirs, files in os.walk(thisDataset):\n",
                "        if analysis_methods.get(\"experiment_name\") in root.split(os.path.sep):## add this condition to avoid data from other experiments\n",
                "            for folder in dirs:\n",
                "                if folder.startswith(\"session\"):\n",
                "                    folder_path = os.path.join(root, folder)\n",
                "                    # Check if the folder contains at least one file with the specified file type\n",
                "                    if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
                "                        session_folders.append(folder_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 2.1: analyse individual animal's optomotor response with curated fictrac tracking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# because the import does not update the new version of python.\n",
                "# Need to restart kernel \n",
                "output0_across_exp=[]\n",
                "output1_across_exp=[]\n",
                "output2_across_exp=[]\n",
                "output3_across_exp=[]\n",
                "output4_across_exp=[]\n",
                "for this_dir in dir_list[23:]:\n",
                "    if \"archive\" in this_dir:\n",
                "        print(f\"skip archive folder for {this_dir}\")\n",
                "        continue\n",
                "    summary,speed,rotation,travel_distance_whole_session=main(this_dir,analysis_methods)\n",
                "    output0_across_exp.append(summary)\n",
                "    output1_across_exp.append(speed)\n",
                "    output2_across_exp.append(rotation)\n",
                "    output3_across_exp.append(travel_distance_whole_session)\n",
                "    output4_across_exp.append(this_dir)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 2.2: Analyse individual animal's optomotor response with the multi-engines module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##this cell start the multi-engines. Make sure to run only once\n",
                "import time\n",
                "import ipyparallel as ipp\n",
                "def show_clusters():\n",
                "    clusters = ipp.ClusterManager().load_clusters() \n",
                "    print(\"{:15} {:^10} {}\".format(\"cluster_id\", \"state\", \"cluster_file\")) \n",
                "    for c in clusters:\n",
                "        cd = clusters[c].to_dict()\n",
                "        cluster_id = cd['cluster']['cluster_id']\n",
                "        controller_state = cd['controller']['state']['state']\n",
                "        cluster_file = getattr(clusters[c], '_trait_values')['cluster_file']\n",
                "        print(\"{:15} {:^10} {}\".format(cluster_id, controller_state, cluster_file))\n",
                "    return cluster_id\n",
                "\n",
                "cluster = ipp.Cluster(n=6)\n",
                "await cluster.start_cluster()\n",
                "cluster_neuropc=show_clusters()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##input cluster_id from previous cell\n",
                "rc = ipp.Client(cluster_id=cluster_neuropc)\n",
                "\n",
                "# Create a DirectView for parallel execution\n",
                "dview = rc.direct_view()\n",
                "\n",
                "# Define a function for parallel processing\n",
                "def process_directory(this_dir, analysis_methods):\n",
                "    from analyse_stimulus_evoked_response import main\n",
                "    # Check if the H5 file (curated dataset) exists\n",
                "    summary,speed,rotation = main(this_dir, analysis_methods)\n",
                "    return (summary,speed,rotation)\n",
                "\n",
                "# Define analysis_methods\n",
                "\n",
                "# Use parallel execution to process directories\n",
                "results = dview.map_sync(process_directory, dir_list, [analysis_methods] * len(dir_list))\n",
                "\n",
                "# Initialize result lists\n",
                "output0_across_exp=[]\n",
                "output1_across_exp=[]\n",
                "output2_across_exp=[]\n",
                "\n",
                "# Collect and organize results\n",
                "for result in results:\n",
                "    if result is not None:\n",
                "        summary,speed,rotation = result\n",
                "        output0_across_exp.append(summary)\n",
                "        output1_across_exp.append(speed)\n",
                "        output2_across_exp.append(rotation)\n",
                "\n",
                "# Now, you have the results collected in the respective lists"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rc.shutdown()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 2.3: plot average behavioural data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "visual_paradigm_name= analysis_methods.get(\"experiment_name\")\n",
                "colormap = np.array(analysis_methods.get(\"graph_colour_code\"))\n",
                "fig2, (ax3, ax4) = plt.subplots(\n",
                "    nrows=1, ncols=2, figsize=(18, 7), tight_layout=True\n",
                ")\n",
                "for i in range(len(output0_across_exp)):\n",
                "    this_animal = output0_across_exp[i]\n",
                "    tmp=this_animal.groupby(\"stim_type\").count()\n",
                "    follow_count_coherence = tmp.index.values\n",
                "    for j in range(len(this_animal.groupby(\"stim_type\"))):\n",
                "        this_coherence=follow_count_coherence[j]\n",
                "        this_response = this_animal.loc[\n",
                "            this_animal[\"stim_type\"] == this_coherence, \"opto_index\"\n",
                "        ].values\n",
                "        # this_coherence = x_axis_value_running_trials[i]\n",
                "        mean_response = np.mean(this_response, axis=0)\n",
                "        sem_response = np.std(this_response, axis=0, ddof=1) / np.sqrt(\n",
                "            this_response.shape[0]\n",
                "        )\n",
                "        ax3.errorbar(\n",
                "            this_coherence,\n",
                "            mean_response,\n",
                "            yerr=sem_response,\n",
                "            c=colormap[5],\n",
                "            fmt=\"o\",\n",
                "            elinewidth=2,\n",
                "            capsize=3,\n",
                "        )\n",
                "    ax3.set_ylim(-1, 1)\n",
                "    ax3.set(\n",
                "        yticks=[-1, 0, 1],\n",
                "        ylabel=\"Optomotor Index\",\n",
                "        xlabel=visual_paradigm_name,)\n",
                "    # ax4.scatter(follow_count_coherence, follow_count, c=colormap[0], marker=\"o\")\n",
                "    # ax4.set_ylim(0, 15)\n",
                "    # ax4.set(\n",
                "    #     yticks=[0, 15],\n",
                "    #     ylabel=\"Follow response (count)\",\n",
                "    #     xticks=[100, 50, 0, -50, -100],\n",
                "    #     xlabel=\"Coherence level (%)\",\n",
                "    # )\n",
                "    ##following one dot (dot lifetime)\n",
                "    ##memory part (30s)\n",
                "    ##interval: rondot\n",
                "    ##continous"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 3: load ephys data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Session 3.0: import packages for analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import time, os, json, warnings\n",
                "import spikeinterface.full as si\n",
                "from raw2si import *\n",
                "from spike_curation import *"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_session_folders(base_directory, file_type, paradigm_name):\n",
                "    session_folders = []\n",
                "\n",
                "    for root, dirs, files in os.walk(base_directory):\n",
                "        # Check if the target folder (e.g., 'apple') is in the root path and the paradigm name is in the root path\n",
                "        if paradigm_name in root.split(os.path.sep):\n",
                "            for folder in dirs:\n",
                "                # Check if the folder name starts with 'session'\n",
                "                if folder.startswith(\"session\"):\n",
                "                    folder_path = os.path.join(root, folder)\n",
                "                    # Check if the folder contains at least one file with the specified file type\n",
                "                    if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
                "                        session_folders.append(folder_path)\n",
                "\n",
                "    return session_folders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# folder_name_start_from=\"sorting\"\n",
                "# for this_dir in dir_list:\n",
                "#     if that_ephys_folder_not_exist(this_dir,folder_name_start_from):\n",
                "#         print(f\"The directory '{this_dir}' does not contain any folders starting with {folder_name_start_from}.\")\n",
                "#         #raw2si(this_dir,analysis_methods)\n",
                "#     else:\n",
                "#         print(f\"The directory '{this_dir}' contains at least one folder starting with {folder_name_start_from}.\")\n",
                "def that_ephys_folder_not_exist(base_directory,pattern):\n",
                "    for item in os.listdir(base_directory):\n",
                "        item_path = os.path.join(base_directory, item)\n",
                "        if os.path.isdir(item_path) and item.startswith(pattern):\n",
                "            return False\n",
                "    return True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##specify an animal root folder to speed up the process because there are a lot of subfolders to look for\n",
                "dir_list = []\n",
                "thisDataset=r\"Y:\\GN25034\"\n",
                "for root, dirs, files in os.walk(thisDataset):\n",
                "    for folder in dirs:\n",
                "        if folder.startswith(\"Record\"):\n",
                "            folder_path=os.path.join(root,folder)\n",
                "            p=Path(folder_path)\n",
                "            #dir_list.append(folder_path.replace(\"\\\\\", \"/\"))\n",
                "            dir_list.append(p.parent.absolute())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dir_list1=dir_list[:2]+dir_list[3:6]+dir_list[7:]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 3.1: create pre-processed dataset and apply an automatic sorter to ephys data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "json_file = \"./analysis_methods_dictionary.json\"\n",
                "\n",
                "if isinstance(json_file, dict):\n",
                "    analysis_methods = json_file\n",
                "else:\n",
                "    with open(json_file, \"r\") as f:\n",
                "        print(f\"load analysis methods from file {json_file}\")\n",
                "        analysis_methods = json.loads(f.read())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#analysis_methods.update({\"save_prepocessed_file\": True,\"load_prepocessed_file\": False,\"save_sorting_file\":True,\"load_sorting_file\":False,\"remove_dead_channels\":False,\"analyse_good_channels_only\":False})\n",
                "for oe_folder in dir_list1:\n",
                "    raw2si(oe_folder, analysis_methods)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "phy_file_pattern=\"params*\"\n",
                "overwrite_existing_phy=analysis_methods.get(\"overwrite_existing_phy\")\n",
                "#analysis_methods.update({\"save_prepocessed_file\": False,\"load_prepocessed_file\": True,\"save_sorting_file\":False,\"load_sorting_file\":True})\n",
                "for oe_folder in dir_list1:\n",
                "    if any(Path(oe_folder).glob(phy_file_pattern)) and overwrite_existing_phy==False:\n",
                "        continue\n",
                "    else:\n",
                "        sorting_analyzer=si2phy(oe_folder, analysis_methods)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 4: visualise the result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "this_sorter=\"kilosort4\"\n",
                "if analysis_methods.get(\"include_MUA\") == True:\n",
                "    cluster_group_interest = [\"noise\"]\n",
                "else:\n",
                "    cluster_group_interest = [\"noise\", \"mua\"]\n",
                "sorter_suffix = generate_sorter_suffix(this_sorter)\n",
                "phy_folder_name = \"phy\" + sorter_suffix\n",
                "analyser_folder_name = \"analyser\" + sorter_suffix\n",
                "sorting_spikes = se.read_phy(\n",
                "    oe_folder / phy_folder_name, exclude_cluster_groups=cluster_group_interest\n",
                ")\n",
                "unit_labels = sorting_spikes.get_property(\"quality\")\n",
                "recording_saved = get_preprocessed_recording(oe_folder,analysis_methods)\n",
                "sorting_analyzer = si.create_sorting_analyzer(\n",
                "    sorting=sorting_spikes,\n",
                "    recording=recording_saved,\n",
                "    sparse=True,  # default\n",
                "    format=\"memory\",  # default\n",
                ")\n",
                "analysis_methods.update({\"load_curated_spikes\": False})\n",
                "analysis_methods.update({\"save_prepocessed_file\": False,\"load_prepocessed_file\": True,\"save_sorting_file\":False,\"load_sorting_file\":True})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "calculate_analyzer_extension(sorting_analyzer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from spikeinterface_gui import run_mainwindow\n",
                "run_mainwindow(sorting_analyzer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run_mainwindow(sorting_analyzer)\n",
                "# %gui qt\n",
                "4# sw.plot_sorting_summary(sorting_analyzer, backend=\"spikeinterface_gui\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 3.2: spike sorting curation and create spike analyser as a database"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for this_dir in dir_list:\n",
                "    if that_ephys_folder_not_exist(this_dir,\"phy\"):\n",
                "        print(f\"The directory '{this_dir}' does not contain any folders starting with phy. That means the manual curation process is not done\")\n",
                "        #spike_curation(this_dir,analysis_methods)\n",
                "    elif that_ephys_folder_not_exist(this_dir,\"analyser\"):\n",
                "        print(f\"The directory '{this_dir}' does not contain any folders starting with analyser. That means the curated data has not been process with anlayser yet\")\n",
                "    else:\n",
                "        print(f\"The directory '{this_dir}' have both folders. Hence it is ready to move on to the next session.\")\n",
                "        continue\n",
                "        #decode_spikes(this_dir,analysis_methods)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 3.3: Sync ephys data with other datasets so that we can plot spike rate in response to the onset of certain events"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 4: Validate the result of automatic sorters"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Session 4.1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "this_dir = r\"D:\\Open Ephys\\2025-03-05_13-45-15\"\n",
                "json_file = \"./analysis_methods_dictionary.json\"\n",
                "oe_folder = Path(this_dir)\n",
                "if isinstance(json_file, dict):\n",
                "    analysis_methods = json_file\n",
                "else:\n",
                "    with open(json_file, \"r\") as f:\n",
                "        print(f\"load analysis methods from file {json_file}\")\n",
                "        analysis_methods = json.loads(f.read())\n",
                "this_experimenter = analysis_methods.get(\"experimenter\")\n",
                "if analysis_methods.get(\"include_MUA\") == True:\n",
                "    cluster_group_interest = [\"noise\"]\n",
                "else:\n",
                "    cluster_group_interest = [\"noise\", \"mua\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sorter_list=[\"kilosort4\",\"spykingcircus2\"]\n",
                "unit_list=[]\n",
                "analyser_list=[]\n",
                "for this_sorter in sorter_list:\n",
                "    sorter_suffix = generate_sorter_suffix(this_sorter)\n",
                "    phy_folder_name = \"phy\" + sorter_suffix\n",
                "    analyser_folder_name = \"analyser\" + sorter_suffix\n",
                "    analyser_list.append(analyser_folder_name)\n",
                "    unit_list.append(si.read_phy(\n",
                "        oe_folder / phy_folder_name, exclude_cluster_groups=cluster_group_interest\n",
                "    ))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "comp2sorters= si.compare_two_sorters(unit_list[0], unit_list[1], sorter_list[0], sorter_list[1])\n",
                "w = si.plot_agreement_matrix(comp2sorters)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "multi_comp = si.compare_multiple_sorters(\n",
                "    sorting_list=unit_list,\n",
                "    name_list=sorter_list,\n",
                "    spiketrain_mode='union',\n",
                "    verbose=True\n",
                ")\n",
                "w = si.plot_multicomparison_agreement(multi_comp) # k sorters means the number of sorters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "w = si.plot_multicomparison_agreement_by_sorter(multi_comp)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "w = si.plot_multicomparison_graph(multi_comp)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from spike_curation import calculate_analyzer_extension,spike_overview\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "analyser_list"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for sorting_spikes,analyser_folder_name in zip(unit_list,analyser_list):\n",
                "    unit_labels = sorting_spikes.get_property(\"quality\")\n",
                "    recording_saved = get_preprocessed_recording(oe_folder,analysis_methods)\n",
                "    analysis_methods.update({\"load_existing_motion_info\": True})\n",
                "    recording_saved=si.astype(recording_saved,np.float32)\n",
                "    recording_corrected_dict=motion_correction_shankbyshank(recording_saved,oe_folder,analysis_methods)\n",
                "    if len(recording_corrected_dict)>1:\n",
                "        recording_for_analysis=si.aggregate_channels(recording_corrected_dict)\n",
                "    else:\n",
                "        recording_for_analysis=recording_corrected_dict[0]\n",
                "    sorting_analyzer = si.create_sorting_analyzer(\n",
                "        sorting=sorting_spikes,\n",
                "        recording=recording_for_analysis,\n",
                "        sparse=True,  # default\n",
                "        format=\"binary_folder\",\n",
                "        folder=oe_folder / analyser_folder_name,\n",
                "        overwrite=True,  # default  # default\n",
                "    )\n",
                "    calculate_analyzer_extension(sorting_analyzer)\n",
                "    metric_names = si.get_quality_metric_list()\n",
                "    qm = si.compute_quality_metrics(sorting_analyzer, metric_names=metric_names, verbose=True)\n",
                "    display(qm)\n",
                "    _, _, _, _ = spike_overview(\n",
                "        oe_folder,\n",
                "        this_sorter,\n",
                "        sorting_spikes,\n",
                "        sorting_analyzer,\n",
                "        recording_for_analysis,\n",
                "        unit_labels,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Running spike sorting with kilosort standalone"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from kilosort import run_kilosort\n",
                "from pathlib import Path\n",
                "import os,json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from kilosort.io import load_ops\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib import gridspec, rcParams"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##specify an animal root folder to speed up the process because there are a lot of subfolders to look for\n",
                "dir_list = []\n",
                "thisDataset=r\"Y:\\GN25034\"\n",
                "for root, dirs, files in os.walk(thisDataset):\n",
                "    for folder in dirs:\n",
                "        if folder.startswith(\"Record\"):\n",
                "            #folder_path=os.path.join(root,folder)\n",
                "            #p=Path(folder_path)\n",
                "            #dir_list.append(folder_path.replace(\"\\\\\", \"/\"))\n",
                "            #dir_list.append(p.parent.resolve())\n",
                "            dir_list.append(Path(root))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_binary_files(base_directory, file_type):\n",
                "    selected_files = []\n",
                "    for root, _, files in os.walk(base_directory):\n",
                "        for file in files:\n",
                "            if file.lower().endswith(file_type) and 'Acquisition_Board-100.acquisition_board' in root:\n",
                "                #p=PurePath(base_directory, root, file)\n",
                "                #selected_files.append(p)\n",
                "                p=Path(root)\n",
                "                selected_files.append(p.joinpath(file))\n",
                "    return selected_files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kilosort_setting_file = \"./p2_parameters_kilosortGUI.json\"\n",
                "probe_file='./P2_RHD2132_openEphys_mapping.prb'\n",
                "bad_channel_list=[23]\n",
                "if isinstance(kilosort_setting_file, dict):\n",
                "    kilosort_methods = kilosort_setting_file\n",
                "else:\n",
                "    with open(kilosort_setting_file, \"r\") as f:\n",
                "        print(f\"load analysis methods from file {kilosort_setting_file}\")\n",
                "        kilosort_methods = json.loads(f.read())\n",
                "settings = {**kilosort_methods['main'], **kilosort_methods['extra']}\n",
                "bad_channel_list=[[23],\n",
                "[23],\n",
                "[],\n",
                "[],\n",
                "[],\n",
                "[],\n",
                "[],\n",
                "[],\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def batch_kilosort_process(settings,probe_file,dir_list,bad_channel_list):\n",
                "    for this_dir,these_bad_channels in zip(dir_list,bad_channel_list):\n",
                "        selected_files=find_binary_files(this_dir, file_type)\n",
                "        if len(selected_files)>1:\n",
                "            print(\"more than one dat file is detected\")\n",
                "        else:\n",
                "            binary_file=selected_files[0]\n",
                "        results_dir = this_dir / 'kilosort4'\n",
                "        run_kilosort(settings=settings,probe_name=probe_file,filename=binary_file,results_dir=results_dir,bad_channels=these_bad_channels)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "###example of running one file\n",
                "file_type=\".dat\"\n",
                "this_dir=dir_list[1]\n",
                "these_bad_channels=[23]\n",
                "selected_files=find_binary_files(this_dir, file_type)\n",
                "if len(selected_files)>1:\n",
                "    print(\"more than one dat file is detected\")\n",
                "else:\n",
                "    binary_file=selected_files[0]\n",
                "\n",
                "results_dir = this_dir / 'kilosort4'\n",
                "ops, st, clu, tF, Wall, similar_templates, is_ref, est_contam_rate, kept_spikes = \\\n",
                "    run_kilosort(\n",
                "        settings=settings,\n",
                "        probe_name=probe_file,\n",
                "        filename=binary_file,\n",
                "        results_dir=results_dir,\n",
                "        bad_channels=these_bad_channels,\n",
                "        )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# outputs saved to results_dir\n",
                "ops = load_ops(results_dir / 'ops.npy')\n",
                "camps = pd.read_csv(results_dir / 'cluster_Amplitude.tsv', sep='\\t')['Amplitude'].values\n",
                "contam_pct = pd.read_csv(results_dir / 'cluster_ContamPct.tsv', sep='\\t')['ContamPct'].values\n",
                "chan_map =  np.load(results_dir / 'channel_map.npy')\n",
                "templates =  np.load(results_dir / 'templates.npy')\n",
                "chan_best = (templates**2).sum(axis=1).argmax(axis=-1)\n",
                "chan_best = chan_map[chan_best]\n",
                "amplitudes = np.load(results_dir / 'amplitudes.npy')\n",
                "st = np.load(results_dir / 'spike_times.npy')\n",
                "clu = np.load(results_dir / 'spike_clusters.npy')\n",
                "firing_rates = np.unique(clu, return_counts=True)[1] * ops['fs'] / st.max()\n",
                "dshift = ops['dshift']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%matplotlib inline\n",
                "display_in_sec=5\n",
                "rcParams['axes.spines.top'] = False\n",
                "rcParams['axes.spines.right'] = False\n",
                "gray = .5 * np.ones(3)\n",
                "\n",
                "fig = plt.figure(figsize=(10,10), dpi=100)\n",
                "grid = gridspec.GridSpec(3, 3, figure=fig, hspace=0.5, wspace=0.5)\n",
                "\n",
                "if settings.get(\"nblocks\")>0:\n",
                "    ax = fig.add_subplot(grid[0,0])\n",
                "    ax.plot(np.arange(0, ops['Nbatches'])*2, dshift)\n",
                "    ax.set_xlabel('time (sec.)')\n",
                "    ax.set_ylabel('drift (um)')\n",
                "\n",
                "ax = fig.add_subplot(grid[0,1:])\n",
                "t0 = 0\n",
                "t1 = np.nonzero(st > ops['fs']*display_in_sec)[0][0]\n",
                "ax.scatter(st[t0:t1]/ops['fs'], chan_best[clu[t0:t1]], s=0.5, color='k', alpha=0.25)\n",
                "ax.set_xlim([0, display_in_sec])\n",
                "ax.set_ylim([chan_map.max(), 0])\n",
                "ax.set_xlabel('time (sec.)')\n",
                "ax.set_ylabel('channel')\n",
                "ax.set_title('spikes from units')\n",
                "\n",
                "ax = fig.add_subplot(grid[1,0])\n",
                "nb=ax.hist(firing_rates, 20, color=gray)\n",
                "ax.set_xlabel('firing rate (Hz)')\n",
                "ax.set_ylabel('# of units')\n",
                "\n",
                "ax = fig.add_subplot(grid[1,1])\n",
                "nb=ax.hist(camps, 20, color=gray)\n",
                "ax.set_xlabel('amplitude')\n",
                "ax.set_ylabel('# of units')\n",
                "\n",
                "ax = fig.add_subplot(grid[1,2])\n",
                "nb=ax.hist(np.minimum(100, contam_pct), np.arange(0,105,5), color=gray)\n",
                "ax.plot([10, 10], [0, nb[0].max()], 'k--')\n",
                "ax.set_xlabel('% contamination')\n",
                "ax.set_ylabel('# of units')\n",
                "ax.set_title('< 10% = good units')\n",
                "\n",
                "for k in range(2):\n",
                "    ax = fig.add_subplot(grid[2,k])\n",
                "    is_ref = contam_pct<10.\n",
                "    ax.scatter(firing_rates[~is_ref], camps[~is_ref], s=3, color='r', label='mua', alpha=0.25)\n",
                "    ax.scatter(firing_rates[is_ref], camps[is_ref], s=3, color='b', label='good', alpha=0.25)\n",
                "    ax.set_ylabel('amplitude (a.u.)')\n",
                "    ax.set_xlabel('firing rate (Hz)')\n",
                "    ax.legend()\n",
                "    if k==1:\n",
                "        ax.set_xscale('log')\n",
                "        ax.set_yscale('log')\n",
                "        ax.set_title('loglog')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "probe = ops['probe']\n",
                "# x and y position of probe sites\n",
                "xc, yc = probe['xc'], probe['yc']\n",
                "nc = 16 # number of channels to show\n",
                "good_units = np.nonzero(contam_pct <= 0.1)[0]\n",
                "mua_units = np.nonzero(contam_pct > 0.1)[0]\n",
                "\n",
                "\n",
                "gstr = ['good', 'mua']\n",
                "for j in range(2):\n",
                "    print(f'~~~~~~~~~~~~~~ {gstr[j]} units ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
                "    print('title = number of spikes from each unit')\n",
                "    units = good_units if j==0 else mua_units\n",
                "    fig = plt.figure(figsize=(12,3), dpi=150)\n",
                "    grid = gridspec.GridSpec(2,20, figure=fig, hspace=0.25, wspace=0.5)\n",
                "\n",
                "    for k in range(40):\n",
                "        wi = units[np.random.randint(len(units))]\n",
                "        wv = templates[wi].copy()\n",
                "        cb = chan_best[wi]\n",
                "        nsp = (clu==wi).sum()\n",
                "\n",
                "        ax = fig.add_subplot(grid[k//20, k%20])\n",
                "        n_chan = wv.shape[-1]\n",
                "        ic0 = max(0, cb-nc//2)\n",
                "        ic1 = min(n_chan, cb+nc//2)\n",
                "        wv = wv[:, ic0:ic1]\n",
                "        x0, y0 = xc[ic0:ic1], yc[ic0:ic1]\n",
                "\n",
                "        amp = 4\n",
                "        for ii, (xi,yi) in enumerate(zip(x0,y0)):\n",
                "            t = np.arange(-wv.shape[0]//2,wv.shape[0]//2,1,'float32')\n",
                "            t /= wv.shape[0] / 20\n",
                "            ax.plot(xi + t, yi + wv[:,ii]*amp, lw=0.5, color='k')\n",
                "\n",
                "        ax.set_title(f'{nsp}', fontsize='small')\n",
                "        ax.axis('off')\n",
                "    plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "kilosort4",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
