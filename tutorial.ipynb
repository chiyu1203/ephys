{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 0.0: import dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries for the analysis\n",
    "import os,sys,json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "##import modules from other folders\n",
    "current_working_directory = Path.cwd()\n",
    "parent_dir = current_working_directory.resolve().parents[0]\n",
    "sys.path.insert(0, str(parent_dir) + \"\\\\utilities\")\n",
    "from useful_tools import select_animals_gpt\n",
    "from data_cleaning import preprocess_fictrac_data\n",
    "sys.path.insert(0, str(parent_dir) + \"\\\\bonfic\")\n",
    "from analyse_stimulus_evoked_response import main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 0.1: Load analysis methods in python dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = \"./analysis_methods_dictionary.json\"\n",
    "with open(json_file, \"r\") as f:\n",
    "    analysis_methods = json.loads(f.read())\n",
    "    \n",
    "sheet_name=\"Zball\"\n",
    "Datasets=\"Z:/DATA/experiment_trackball_Optomotor\"\n",
    "thisDataset = f\"{Datasets}/{sheet_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 0.2: check methods to use and whether some methods should be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 0.3: Load animal directory as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this cell searches for a folder with a specified experiment_name under the dataset path and list up all the csv file in that folder.\n",
    "## In this project, we usually have one csv file in that folder so there is no confusion\n",
    "dir_list = []\n",
    "file_type=\".dat\"\n",
    "for root, dirs, files in os.walk(thisDataset):\n",
    "    if analysis_methods.get(\"experiment_name\") in root.split(\n",
    "        os.path.sep\n",
    "    ):  ## add this condition to avoid data from other experiments\n",
    "        for folder in dirs:\n",
    "            if folder.startswith(\"session\"):\n",
    "                folder_path=os.path.join(root,folder)\n",
    "                if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
    "                    dir_list.append(folder_path.replace(\"\\\\\", \"/\"))\n",
    "\n",
    "\n",
    "print(f\"these directories are found {dir_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list[23:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_session_folders(base_directory, file_type, paradigm_name):\n",
    "    session_folders = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        # Check if the target folder (e.g., 'apple') is in the root path and the paradigm name is in the root path\n",
    "        if paradigm_name in root.split(os.path.sep):\n",
    "            for folder in dirs:\n",
    "                # Check if the folder name starts with 'session'\n",
    "                if folder.startswith(\"session\"):\n",
    "                    folder_path = os.path.join(root, folder)\n",
    "                    # Check if the folder contains at least one file with the specified file type\n",
    "                    if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
    "                        session_folders.append(folder_path)\n",
    "\n",
    "    return session_folders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = thisDataset\n",
    "file_type = \".dat\"\n",
    "paradigm_name = analysis_methods.get(\"experiment_name\")\n",
    "\n",
    "session_folders = find_session_folders(base_directory, file_type, paradigm_name)\n",
    "\n",
    "print(f\"These directories are found: {session_folders}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 1.0: Create fictrac curated dataset based on the list of directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the import does not update the new version of python.\n",
    "# Need to restart kernel \n",
    "for this_dir in dir_list:\n",
    "    if \"database_curated.pickle\" in os.listdir(this_dir):\n",
    "        print(f\"curated fictrac data found in {this_dir}. Skip this file\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"no curated fictrac data in {this_dir}. Create curated file\")\n",
    "        preprocess_fictrac_data(this_dir,analysis_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 1.5: load particular animals into directory list for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your Excel file\n",
    "dir_list = []\n",
    "file_type=\".pickle\"\n",
    "using_google_sheet=True\n",
    "sheet_name = \"VCCball\"\n",
    "experiment_name=analysis_methods.get(\"experiment_name\")\n",
    "if analysis_methods.get(\"load_experiment_condition_from_database\") == True:\n",
    "    if using_google_sheet==True:\n",
    "        database_id = \"1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8\"\n",
    "                #https://docs.google.com/spreadsheets/d/1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8/edit?usp=sharing\n",
    "        url = f\"https://docs.google.com/spreadsheets/d/{database_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "        #df = pd.read_excel(url, engine='openpyxl')## use this function if the file is not google sheet but uploaded excel file\n",
    "\n",
    "        df = pd.read_csv(url)\n",
    "    else:\n",
    "        excel_file_path = \"Z:/DATA/experiment_trackball_Optomotor/Locusts Management.xlsx\"\n",
    "        print(f\"using a database {excel_file_path} from the server but this file might be outdated\")\n",
    "        # Create a 'with' statement to open and read the Excel file\n",
    "        with pd.ExcelFile(excel_file_path) as xls:\n",
    "            # Read the Excel sheet into a DataFrame with the sheet name (folder name)\n",
    "            df = pd.read_excel(xls, sheet_name)\n",
    "        ##list up the conditions and answers as strings for input argument to select animal. One condition must pair with one answer\n",
    "    if analysis_methods.get(\"select_animals_by_condition\") == True:\n",
    "        animal_of_interest=select_animals_gpt(df,\"Experimenter\",\"NS\")\n",
    "        #print(animal_of_interest)\n",
    "    else:\n",
    "        animal_of_interest=df\n",
    "    ID_array=animal_of_interest[\"ID\"].values\n",
    "    dir_list = [\n",
    "    root.replace(\"\\\\\", \"/\")\n",
    "    for root, dirs, files in os.walk(thisDataset)\n",
    "    if any(ID in root for ID in ID_array)\n",
    "    and experiment_name in root.split(os.path.sep)\n",
    "    and any(name.endswith(file_type) for name in files)\n",
    "\n",
    "\n",
    "    \n",
    "]\n",
    "else:\n",
    "    ## this cell searches for a folder with a specified experiment_name under the dataset path and list up all the hdf5 file in that folder.\n",
    "    ## However,some changes need to be made once we do sleap or deeplabcut where there are more than one H5 file generated\n",
    "    for root, dirs, files in os.walk(thisDataset):\n",
    "        if analysis_methods.get(\"experiment_name\") in root.split(os.path.sep):## add this condition to avoid data from other experiments\n",
    "            for folder in dirs:\n",
    "                if folder.startswith(\"session\"):\n",
    "                    folder_path = os.path.join(root, folder)\n",
    "                    # Check if the folder contains at least one file with the specified file type\n",
    "                    if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
    "                        session_folders.append(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 2.1: analyse individual animal's optomotor response with curated fictrac tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the import does not update the new version of python.\n",
    "# Need to restart kernel \n",
    "output0_across_exp=[]\n",
    "output1_across_exp=[]\n",
    "output2_across_exp=[]\n",
    "output3_across_exp=[]\n",
    "output4_across_exp=[]\n",
    "for this_dir in dir_list[23:]:\n",
    "    if \"archive\" in this_dir:\n",
    "        print(f\"skip archive folder for {this_dir}\")\n",
    "        continue\n",
    "    summary,speed,rotation,travel_distance_whole_session=main(this_dir,analysis_methods)\n",
    "    output0_across_exp.append(summary)\n",
    "    output1_across_exp.append(speed)\n",
    "    output2_across_exp.append(rotation)\n",
    "    output3_across_exp.append(travel_distance_whole_session)\n",
    "    output4_across_exp.append(this_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 2.2: Analyse individual animal's optomotor response with the multi-engines module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this cell start the multi-engines. Make sure to run only once\n",
    "import time\n",
    "import ipyparallel as ipp\n",
    "def show_clusters():\n",
    "    clusters = ipp.ClusterManager().load_clusters() \n",
    "    print(\"{:15} {:^10} {}\".format(\"cluster_id\", \"state\", \"cluster_file\")) \n",
    "    for c in clusters:\n",
    "        cd = clusters[c].to_dict()\n",
    "        cluster_id = cd['cluster']['cluster_id']\n",
    "        controller_state = cd['controller']['state']['state']\n",
    "        cluster_file = getattr(clusters[c], '_trait_values')['cluster_file']\n",
    "        print(\"{:15} {:^10} {}\".format(cluster_id, controller_state, cluster_file))\n",
    "    return cluster_id\n",
    "\n",
    "cluster = ipp.Cluster(n=6)\n",
    "await cluster.start_cluster()\n",
    "cluster_neuropc=show_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##input cluster_id from previous cell\n",
    "rc = ipp.Client(cluster_id=cluster_neuropc)\n",
    "\n",
    "# Create a DirectView for parallel execution\n",
    "dview = rc.direct_view()\n",
    "\n",
    "# Define a function for parallel processing\n",
    "def process_directory(this_dir, analysis_methods):\n",
    "    from analyse_stimulus_evoked_response import main\n",
    "    # Check if the H5 file (curated dataset) exists\n",
    "    summary,speed,rotation = main(this_dir, analysis_methods)\n",
    "    return (summary,speed,rotation)\n",
    "\n",
    "# Define analysis_methods\n",
    "\n",
    "# Use parallel execution to process directories\n",
    "results = dview.map_sync(process_directory, dir_list, [analysis_methods] * len(dir_list))\n",
    "\n",
    "# Initialize result lists\n",
    "output0_across_exp=[]\n",
    "output1_across_exp=[]\n",
    "output2_across_exp=[]\n",
    "\n",
    "# Collect and organize results\n",
    "for result in results:\n",
    "    if result is not None:\n",
    "        summary,speed,rotation = result\n",
    "        output0_across_exp.append(summary)\n",
    "        output1_across_exp.append(speed)\n",
    "        output2_across_exp.append(rotation)\n",
    "\n",
    "# Now, you have the results collected in the respective lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 2.3: plot average behavioural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_paradigm_name= analysis_methods.get(\"experiment_name\")\n",
    "colormap = np.array(analysis_methods.get(\"graph_colour_code\"))\n",
    "fig2, (ax3, ax4) = plt.subplots(\n",
    "    nrows=1, ncols=2, figsize=(18, 7), tight_layout=True\n",
    ")\n",
    "for i in range(len(output0_across_exp)):\n",
    "    this_animal = output0_across_exp[i]\n",
    "    tmp=this_animal.groupby(\"stim_type\").count()\n",
    "    follow_count_coherence = tmp.index.values\n",
    "    for j in range(len(this_animal.groupby(\"stim_type\"))):\n",
    "        this_coherence=follow_count_coherence[j]\n",
    "        this_response = this_animal.loc[\n",
    "            this_animal[\"stim_type\"] == this_coherence, \"opto_index\"\n",
    "        ].values\n",
    "        # this_coherence = x_axis_value_running_trials[i]\n",
    "        mean_response = np.mean(this_response, axis=0)\n",
    "        sem_response = np.std(this_response, axis=0, ddof=1) / np.sqrt(\n",
    "            this_response.shape[0]\n",
    "        )\n",
    "        ax3.errorbar(\n",
    "            this_coherence,\n",
    "            mean_response,\n",
    "            yerr=sem_response,\n",
    "            c=colormap[5],\n",
    "            fmt=\"o\",\n",
    "            elinewidth=2,\n",
    "            capsize=3,\n",
    "        )\n",
    "    ax3.set_ylim(-1, 1)\n",
    "    ax3.set(\n",
    "        yticks=[-1, 0, 1],\n",
    "        ylabel=\"Optomotor Index\",\n",
    "        xlabel=visual_paradigm_name,)\n",
    "    # ax4.scatter(follow_count_coherence, follow_count, c=colormap[0], marker=\"o\")\n",
    "    # ax4.set_ylim(0, 15)\n",
    "    # ax4.set(\n",
    "    #     yticks=[0, 15],\n",
    "    #     ylabel=\"Follow response (count)\",\n",
    "    #     xticks=[100, 50, 0, -50, -100],\n",
    "    #     xlabel=\"Coherence level (%)\",\n",
    "    # )\n",
    "    ##following one dot (dot lifetime)\n",
    "    ##memory part (30s)\n",
    "    ##interval: rondot\n",
    "    ##continous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 3: load ephys data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 3.0: import packages for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time, os, json, warnings\n",
    "import spikeinterface.full as si\n",
    "from raw2si import *\n",
    "from spike_curation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## list up all directory containing a dat file under a folder named after certain experiments. Only useful when accessing the database in SSD or Uni file Cloud\n",
    "# thisDataset=r\"D:\\Open Ephys\"\n",
    "# dir_list = []\n",
    "# file_type=\".dat\"\n",
    "# for root, dirs, files in os.walk(thisDataset):\n",
    "#     if analysis_methods.get(\"experiment_name\") in root.split(\n",
    "#         os.path.sep\n",
    "#     ):  ## add this condition to avoid data from other experiments\n",
    "#         for folder in dirs:\n",
    "#             if folder.startswith(\"Record Node\"):\n",
    "#                 dir_list.append(root.replace(\"\\\\\", \"/\"))\n",
    "\n",
    "thisDataset=r\"D:\\Open Ephys\"\n",
    "dir_list = []\n",
    "file_type=\".dat\"\n",
    "for root, dirs, files in os.walk(thisDataset):\n",
    "    for folder in dirs:\n",
    "        if folder.startswith(\"Record Node\"):\n",
    "            dir_list.append(root.replace(\"\\\\\", \"/\"))\n",
    "\n",
    "print(f\"these directories are found {dir_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 3.1: create pre-processed dataset and apply an automatic sorter to ephys data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def that_ephys_folder_not_exist(base_directory,pattern):\n",
    "    for item in os.listdir(base_directory):\n",
    "        item_path = os.path.join(base_directory, item)\n",
    "        if os.path.isdir(item_path) and item.startswith(pattern):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_name_start_from=\"sorting\"\n",
    "# for this_dir in dir_list:\n",
    "#     if that_ephys_folder_not_exist(this_dir,folder_name_start_from):\n",
    "#         print(f\"The directory '{this_dir}' does not contain any folders starting with {folder_name_start_from}.\")\n",
    "#         #raw2si(this_dir,analysis_methods)\n",
    "#     else:\n",
    "#         print(f\"The directory '{this_dir}' contains at least one folder starting with {folder_name_start_from}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this_dir = r\"D:\\Open Ephys\\2025-03-05_13-45-15\"\n",
    "this_dir = r\"D:\\Open Ephys\\2025-05-12_19-17-47\"\n",
    "json_file = \"./analysis_methods_dictionary.json\"\n",
    "oe_folder = Path(this_dir)\n",
    "if isinstance(json_file, dict):\n",
    "    analysis_methods = json_file\n",
    "else:\n",
    "    with open(json_file, \"r\") as f:\n",
    "        print(f\"load analysis methods from file {json_file}\")\n",
    "        analysis_methods = json.loads(f.read())\n",
    "analysis_methods.update({\"save_prepocessed_file\": True,\"load_prepocessed_file\": False,\"save_sorting_file\":True,\"load_sorting_file\":False,\"remove_dead_channels\":False,\"analyse_good_channels_only\":False})\n",
    "raw2si(this_dir, analysis_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_methods.update({\"save_prepocessed_file\": False,\"load_prepocessed_file\": True,\"save_sorting_file\":False,\"load_sorting_file\":True})\n",
    "si2phy(this_dir, analysis_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 3.2: spike sorting curation and create spike analyser as a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_dir in dir_list:\n",
    "    if that_ephys_folder_not_exist(this_dir,\"phy\"):\n",
    "        print(f\"The directory '{this_dir}' does not contain any folders starting with phy. That means the manual curation process is not done\")\n",
    "        #spike_curation(this_dir,analysis_methods)\n",
    "    elif that_ephys_folder_not_exist(this_dir,\"analyser\"):\n",
    "        print(f\"The directory '{this_dir}' does not contain any folders starting with analyser. That means the curated data has not been process with anlayser yet\")\n",
    "    else:\n",
    "        print(f\"The directory '{this_dir}' have both folders. Hence it is ready to move on to the next session.\")\n",
    "        continue\n",
    "        #decode_spikes(this_dir,analysis_methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 3.3: Sync ephys data with other datasets so that we can plot spike rate in response to the onset of certain events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 4: Validate the result of automatic sorters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Session 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_dir = r\"D:\\Open Ephys\\2025-03-05_13-45-15\"\n",
    "json_file = \"./analysis_methods_dictionary.json\"\n",
    "oe_folder = Path(this_dir)\n",
    "if isinstance(json_file, dict):\n",
    "    analysis_methods = json_file\n",
    "else:\n",
    "    with open(json_file, \"r\") as f:\n",
    "        print(f\"load analysis methods from file {json_file}\")\n",
    "        analysis_methods = json.loads(f.read())\n",
    "this_experimenter = analysis_methods.get(\"experimenter\")\n",
    "if analysis_methods.get(\"include_MUA\") == True:\n",
    "    cluster_group_interest = [\"noise\"]\n",
    "else:\n",
    "    cluster_group_interest = [\"noise\", \"mua\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorter_list=[\"kilosort4\",\"spykingcircus2\"]\n",
    "unit_list=[]\n",
    "analyser_list=[]\n",
    "for this_sorter in sorter_list:\n",
    "    sorter_suffix = generate_sorter_suffix(this_sorter)\n",
    "    phy_folder_name = \"phy\" + sorter_suffix\n",
    "    analyser_folder_name = \"analyser\" + sorter_suffix\n",
    "    analyser_list.append(analyser_folder_name)\n",
    "    unit_list.append(si.read_phy(\n",
    "        oe_folder / phy_folder_name, exclude_cluster_groups=cluster_group_interest\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp2sorters= si.compare_two_sorters(unit_list[0], unit_list[1], sorter_list[0], sorter_list[1])\n",
    "w = si.plot_agreement_matrix(comp2sorters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_comp = si.compare_multiple_sorters(\n",
    "    sorting_list=unit_list,\n",
    "    name_list=sorter_list,\n",
    "    spiketrain_mode='union',\n",
    "    verbose=True\n",
    ")\n",
    "w = si.plot_multicomparison_agreement(multi_comp) # k sorters means the number of sorters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = si.plot_multicomparison_agreement_by_sorter(multi_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = si.plot_multicomparison_graph(multi_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spike_curation import calculate_analyzer_extension,spike_overview\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sorting_spikes,analyser_folder_name in zip(unit_list,analyser_list):\n",
    "    unit_labels = sorting_spikes.get_property(\"quality\")\n",
    "    recording_saved = get_preprocessed_recording(oe_folder,analysis_methods)\n",
    "    analysis_methods.update({\"load_existing_motion_info\": True})\n",
    "    recording_saved=si.astype(recording_saved,np.float32)\n",
    "    recording_corrected_dict=motion_correction_shankbyshank(recording_saved,oe_folder,analysis_methods)\n",
    "    if len(recording_corrected_dict)>1:\n",
    "        recording_for_analysis=si.aggregate_channels(recording_corrected_dict)\n",
    "    else:\n",
    "        recording_for_analysis=recording_corrected_dict[0]\n",
    "    sorting_analyzer = si.create_sorting_analyzer(\n",
    "        sorting=sorting_spikes,\n",
    "        recording=recording_for_analysis,\n",
    "        sparse=True,  # default\n",
    "        format=\"binary_folder\",\n",
    "        folder=oe_folder / analyser_folder_name,\n",
    "        overwrite=True,  # default  # default\n",
    "    )\n",
    "    calculate_analyzer_extension(sorting_analyzer)\n",
    "    metric_names = si.get_quality_metric_list()\n",
    "    qm = si.compute_quality_metrics(sorting_analyzer, metric_names=metric_names, verbose=True)\n",
    "    display(qm)\n",
    "    _, _, _, _ = spike_overview(\n",
    "        oe_folder,\n",
    "        this_sorter,\n",
    "        sorting_spikes,\n",
    "        sorting_analyzer,\n",
    "        recording_for_analysis,\n",
    "        unit_labels,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike_interface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
