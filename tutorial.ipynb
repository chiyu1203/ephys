{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 0.0: import dependancies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## import libraries for the analysis\n",
                "import os,sys,json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "##import modules from other folders\n",
                "current_working_directory = Path.cwd()\n",
                "parent_dir = current_working_directory.resolve().parents[0]\n",
                "sys.path.insert(0, str(parent_dir) + \"\\\\utilities\")\n",
                "from useful_tools import select_animals_gpt\n",
                "from data_cleaning import preprocess_fictrac_data\n",
                "sys.path.insert(0, str(parent_dir) + \"\\\\bonfic\")\n",
                "from analyse_stimulus_evoked_response import main"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 0.1: Load analysis methods in python dictionary form"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "json_file = \"./analysis_methods_dictionary.json\"\n",
                "with open(json_file, \"r\") as f:\n",
                "    analysis_methods = json.loads(f.read())\n",
                "    \n",
                "sheet_name=\"Zball\"\n",
                "Datasets=\"Z:/DATA/experiment_trackball_Optomotor\"\n",
                "thisDataset = f\"{Datasets}/{sheet_name}\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 0.2: check methods to use and whether some methods should be updated"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 0.3: Load animal directory as a list"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## this cell searches for a folder with a specified experiment_name under the dataset path and list up all the csv file in that folder.\n",
                "## In this project, we usually have one csv file in that folder so there is no confusion\n",
                "dir_list = []\n",
                "file_type=\".dat\"\n",
                "for root, dirs, files in os.walk(thisDataset):\n",
                "    if analysis_methods.get(\"experiment_name\") in root.split(\n",
                "        os.path.sep\n",
                "    ):  ## add this condition to avoid data from other experiments\n",
                "        for folder in dirs:\n",
                "            if folder.startswith(\"session\"):\n",
                "                folder_path=os.path.join(root,folder)\n",
                "                if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
                "                    dir_list.append(folder_path.replace(\"\\\\\", \"/\"))\n",
                "\n",
                "\n",
                "print(f\"these directories are found {dir_list}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_session_folders(base_directory, file_type, paradigm_name):\n",
                "    session_folders = []\n",
                "\n",
                "    for root, dirs, files in os.walk(base_directory):\n",
                "        # Check if the target folder (e.g., 'apple') is in the root path and the paradigm name is in the root path\n",
                "        if paradigm_name in root.split(os.path.sep):\n",
                "            for folder in dirs:\n",
                "                # Check if the folder name starts with 'session'\n",
                "                if folder.startswith(\"session\"):\n",
                "                    folder_path = os.path.join(root, folder)\n",
                "                    # Check if the folder contains at least one file with the specified file type\n",
                "                    if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
                "                        session_folders.append(folder_path)\n",
                "\n",
                "    return session_folders\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "base_directory = thisDataset\n",
                "file_type = \".dat\"\n",
                "paradigm_name = analysis_methods.get(\"experiment_name\")\n",
                "\n",
                "session_folders = find_session_folders(base_directory, file_type, paradigm_name)\n",
                "\n",
                "print(f\"These directories are found: {session_folders}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 1.0: Create fictrac curated dataset based on the list of directories"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# because the import does not update the new version of python.\n",
                "# Need to restart kernel \n",
                "for this_dir in dir_list:\n",
                "    if \"database_curated.pickle\" in os.listdir(this_dir):\n",
                "        print(f\"curated fictrac data found in {this_dir}. Skip this file\")\n",
                "        continue\n",
                "    else:\n",
                "        print(f\"no curated fictrac data in {this_dir}. Create curated file\")\n",
                "        preprocess_fictrac_data(this_dir,analysis_methods)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 1.5: load particular animals into directory list for further analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the path to your Excel file\n",
                "dir_list = []\n",
                "file_type=\".pickle\"\n",
                "using_google_sheet=True\n",
                "sheet_name = \"VCCball\"\n",
                "experiment_name=analysis_methods.get(\"experiment_name\")\n",
                "if analysis_methods.get(\"load_experiment_condition_from_database\") == True:\n",
                "    if using_google_sheet==True:\n",
                "        database_id = \"1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8\"\n",
                "                #https://docs.google.com/spreadsheets/d/1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8/edit?usp=sharing\n",
                "        url = f\"https://docs.google.com/spreadsheets/d/{database_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
                "        #df = pd.read_excel(url, engine='openpyxl')## use this function if the file is not google sheet but uploaded excel file\n",
                "\n",
                "        df = pd.read_csv(url)\n",
                "    else:\n",
                "        excel_file_path = \"Z:/DATA/experiment_trackball_Optomotor/Locusts Management.xlsx\"\n",
                "        print(f\"using a database {excel_file_path} from the server but this file might be outdated\")\n",
                "        # Create a 'with' statement to open and read the Excel file\n",
                "        with pd.ExcelFile(excel_file_path) as xls:\n",
                "            # Read the Excel sheet into a DataFrame with the sheet name (folder name)\n",
                "            df = pd.read_excel(xls, sheet_name)\n",
                "        ##list up the conditions and answers as strings for input argument to select animal. One condition must pair with one answer\n",
                "    if analysis_methods.get(\"select_animals_by_condition\") == True:\n",
                "        animal_of_interest=select_animals_gpt(df,\"Experimenter\",\"NS\")\n",
                "        #print(animal_of_interest)\n",
                "    else:\n",
                "        animal_of_interest=df\n",
                "    ID_array=animal_of_interest[\"ID\"].values\n",
                "    dir_list = [\n",
                "    root.replace(\"\\\\\", \"/\")\n",
                "    for root, dirs, files in os.walk(thisDataset)\n",
                "    if any(ID in root for ID in ID_array)\n",
                "    and experiment_name in root.split(os.path.sep)\n",
                "    and any(name.endswith(file_type) for name in files)\n",
                "\n",
                "\n",
                "    \n",
                "]\n",
                "else:\n",
                "    ## this cell searches for a folder with a specified experiment_name under the dataset path and list up all the hdf5 file in that folder.\n",
                "    ## However,some changes need to be made once we do sleap or deeplabcut where there are more than one H5 file generated\n",
                "    for root, dirs, files in os.walk(thisDataset):\n",
                "        if analysis_methods.get(\"experiment_name\") in root.split(os.path.sep):## add this condition to avoid data from other experiments\n",
                "            for folder in dirs:\n",
                "                if folder.startswith(\"session\"):\n",
                "                    folder_path = os.path.join(root, folder)\n",
                "                    # Check if the folder contains at least one file with the specified file type\n",
                "                    if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
                "                        session_folders.append(folder_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 2.1: analyse individual animal's optomotor response with curated fictrac tracking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# because the import does not update the new version of python.\n",
                "# Need to restart kernel \n",
                "output0_across_exp=[]\n",
                "output1_across_exp=[]\n",
                "output2_across_exp=[]\n",
                "output3_across_exp=[]\n",
                "output4_across_exp=[]\n",
                "for this_dir in dir_list[23:]:\n",
                "    if \"archive\" in this_dir:\n",
                "        print(f\"skip archive folder for {this_dir}\")\n",
                "        continue\n",
                "    summary,speed,rotation,travel_distance_whole_session=main(this_dir,analysis_methods)\n",
                "    output0_across_exp.append(summary)\n",
                "    output1_across_exp.append(speed)\n",
                "    output2_across_exp.append(rotation)\n",
                "    output3_across_exp.append(travel_distance_whole_session)\n",
                "    output4_across_exp.append(this_dir)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 2.2: Analyse individual animal's optomotor response with the multi-engines module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##this cell start the multi-engines. Make sure to run only once\n",
                "import time\n",
                "import ipyparallel as ipp\n",
                "def show_clusters():\n",
                "    clusters = ipp.ClusterManager().load_clusters() \n",
                "    print(\"{:15} {:^10} {}\".format(\"cluster_id\", \"state\", \"cluster_file\")) \n",
                "    for c in clusters:\n",
                "        cd = clusters[c].to_dict()\n",
                "        cluster_id = cd['cluster']['cluster_id']\n",
                "        controller_state = cd['controller']['state']['state']\n",
                "        cluster_file = getattr(clusters[c], '_trait_values')['cluster_file']\n",
                "        print(\"{:15} {:^10} {}\".format(cluster_id, controller_state, cluster_file))\n",
                "    return cluster_id\n",
                "\n",
                "cluster = ipp.Cluster(n=6)\n",
                "await cluster.start_cluster()\n",
                "cluster_neuropc=show_clusters()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##input cluster_id from previous cell\n",
                "rc = ipp.Client(cluster_id=cluster_neuropc)\n",
                "\n",
                "# Create a DirectView for parallel execution\n",
                "dview = rc.direct_view()\n",
                "\n",
                "# Define a function for parallel processing\n",
                "def process_directory(this_dir, analysis_methods):\n",
                "    from analyse_stimulus_evoked_response import main\n",
                "    # Check if the H5 file (curated dataset) exists\n",
                "    summary,speed,rotation = main(this_dir, analysis_methods)\n",
                "    return (summary,speed,rotation)\n",
                "\n",
                "# Define analysis_methods\n",
                "\n",
                "# Use parallel execution to process directories\n",
                "results = dview.map_sync(process_directory, dir_list, [analysis_methods] * len(dir_list))\n",
                "\n",
                "# Initialize result lists\n",
                "output0_across_exp=[]\n",
                "output1_across_exp=[]\n",
                "output2_across_exp=[]\n",
                "\n",
                "# Collect and organize results\n",
                "for result in results:\n",
                "    if result is not None:\n",
                "        summary,speed,rotation = result\n",
                "        output0_across_exp.append(summary)\n",
                "        output1_across_exp.append(speed)\n",
                "        output2_across_exp.append(rotation)\n",
                "\n",
                "# Now, you have the results collected in the respective lists"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rc.shutdown()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 2.3: plot average behavioural data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "visual_paradigm_name= analysis_methods.get(\"experiment_name\")\n",
                "colormap = np.array(analysis_methods.get(\"graph_colour_code\"))\n",
                "fig2, (ax3, ax4) = plt.subplots(\n",
                "    nrows=1, ncols=2, figsize=(18, 7), tight_layout=True\n",
                ")\n",
                "for i in range(len(output0_across_exp)):\n",
                "    this_animal = output0_across_exp[i]\n",
                "    tmp=this_animal.groupby(\"stim_type\").count()\n",
                "    follow_count_coherence = tmp.index.values\n",
                "    for j in range(len(this_animal.groupby(\"stim_type\"))):\n",
                "        this_coherence=follow_count_coherence[j]\n",
                "        this_response = this_animal.loc[\n",
                "            this_animal[\"stim_type\"] == this_coherence, \"opto_index\"\n",
                "        ].values\n",
                "        # this_coherence = x_axis_value_running_trials[i]\n",
                "        mean_response = np.mean(this_response, axis=0)\n",
                "        sem_response = np.std(this_response, axis=0, ddof=1) / np.sqrt(\n",
                "            this_response.shape[0]\n",
                "        )\n",
                "        ax3.errorbar(\n",
                "            this_coherence,\n",
                "            mean_response,\n",
                "            yerr=sem_response,\n",
                "            c=colormap[5],\n",
                "            fmt=\"o\",\n",
                "            elinewidth=2,\n",
                "            capsize=3,\n",
                "        )\n",
                "    ax3.set_ylim(-1, 1)\n",
                "    ax3.set(\n",
                "        yticks=[-1, 0, 1],\n",
                "        ylabel=\"Optomotor Index\",\n",
                "        xlabel=visual_paradigm_name,)\n",
                "    # ax4.scatter(follow_count_coherence, follow_count, c=colormap[0], marker=\"o\")\n",
                "    # ax4.set_ylim(0, 15)\n",
                "    # ax4.set(\n",
                "    #     yticks=[0, 15],\n",
                "    #     ylabel=\"Follow response (count)\",\n",
                "    #     xticks=[100, 50, 0, -50, -100],\n",
                "    #     xlabel=\"Coherence level (%)\",\n",
                "    # )\n",
                "    ##following one dot (dot lifetime)\n",
                "    ##memory part (30s)\n",
                "    ##interval: rondot\n",
                "    ##continous"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 3: load ephys data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Session 3.0: import packages for analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%reload_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import time, os, json, warnings\n",
                "#import spikeinterface.full as si\n",
                "import spikeinterface.core as si\n",
                "import spikeinterface.extractors as se\n",
                "import spikeinterface.preprocessing as spre\n",
                "import spikeinterface.postprocessing as spost\n",
                "import spikeinterface.sorters as ss\n",
                "import spikeinterface.qualitymetrics as sq\n",
                "import spikeinterface.exporters as sep\n",
                "from spikeinterface.curation import load_curation, apply_curation\n",
                "from raw2si import *\n",
                "from spike_curation import *\n",
                "from spikeinterface_gui import run_mainwindow"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 3.1: create pre-processed dataset and apply an automatic sorter to ephys data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "##specify an animal root folder to speed up the process because there are a lot of subfolders to look for\n",
                "dir_list = []\n",
                "thisDataset=r\"Y:\\GN25060\"\n",
                "for root, dirs, files in os.walk(thisDataset):\n",
                "    for folder in dirs:\n",
                "        if folder.startswith(\"Record\"):\n",
                "            dir_list.append(Path(root))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[WindowsPath('Y:/GN25060/251130/coherence/session1/2025-11-30_14-25-01')]"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dir_list[:1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "json_file = \"./analysis_methods_dictionary.json\"\n",
                "\n",
                "if isinstance(json_file, dict):\n",
                "    analysis_methods = json_file\n",
                "else:\n",
                "    with open(json_file, \"r\") as f:\n",
                "        print(f\"load analysis methods from file {json_file}\")\n",
                "        analysis_methods = json.loads(f.read())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#analysis_methods.update({\"save_prepocessed_file\": True,\"load_prepocessed_file\": False,\"save_sorting_file\":True,\"load_sorting_file\":False,\"remove_dead_channels\":False,\"analyse_good_channels_only\":False})\n",
                "for oe_folder in dir_list:\n",
                "    if type(oe_folder)==str:\n",
                "        oe_folder=Path(oe_folder)\n",
                "    print(f\"processing {oe_folder}\")\n",
                "    raw2si(oe_folder, analysis_methods)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "phy_file_pattern=\"params*\"\n",
                "#it takes 5 hours to do postprocessing on \\GN25060\\251130\\coherence\\session1\\2025-11-30_14-25-01\\\n",
                "overwrite_existing_phy=analysis_methods.get(\"overwrite_existing_phy\")\n",
                "#analysis_methods.update({\"save_prepocessed_file\": False,\"load_prepocessed_file\": True,\"save_sorting_file\":False,\"load_sorting_file\":True})\n",
                "for oe_folder in dir_list:\n",
                "    if any(Path(oe_folder).glob(phy_file_pattern)) and overwrite_existing_phy==False:\n",
                "        continue\n",
                "    else:\n",
                "        sorting_analyzer=si2phy(oe_folder, analysis_methods)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Session 3.2: analyse single file with multiple sorters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#oe_folder=r'Y:\\GN25037\\250922\\looming\\session2\\2025-09-22_15-59-41'\n",
                "#oe_folder=r'Y:\\GN25033\\250906\\looming\\session1\\2025-09-06_18-42-24'\n",
                "dir_list=[r'Y:\\GN25044\\251012\\looming\\session1\\2025-10-12_14-22-01',r'Y:\\GN25045\\251013\\looming\\session1\\2025-10-13_11-16-41',r'Y:\\GN25046\\251018\\looming\\session1\\2025-10-18_16-34-27',r'Y:\\GN25048\\251019\\looming\\session1\\2025-10-19_18-50-34',r'Y:\\GN25033\\250906\\looming\\session1\\2025-09-06_18-42-24']\n",
                "analysis_methods.update({\"overwrite_curated_dataset\": True})\n",
                "sorter_list=[\"kilosort4\",\"spykingcircus2\"]\n",
                "for oe_folder in dir_list:\n",
                "    for this_sorter in sorter_list:\n",
                "        analysis_methods.update({'sorter_name': this_sorter})\n",
                "        raw2si(oe_folder, analysis_methods)\n",
                "        _=si2phy(oe_folder, analysis_methods)    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 4: visualise the result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "this_sorter=analysis_methods.get(\"sorter_name\")\n",
                "oe_folder=r'Y:\\GN25060\\251130\\coherence\\session1\\2025-11-30_14-25-01'\n",
                "if type(oe_folder)==str:\n",
                "    oe_folder=Path(oe_folder)\n",
                "\n",
                "if analysis_methods.get(\"include_MUA\") == True:\n",
                "    cluster_group_interest = [\"noise\"]\n",
                "else:\n",
                "    cluster_group_interest = [\"noise\", \"mua\"]\n",
                "sorter_suffix = generate_sorter_suffix(this_sorter)\n",
                "phy_folder_name = \"phy\" + sorter_suffix\n",
                "analyser_folder_name = \"analyser\" + sorter_suffix\n",
                "sorting_spikes = se.read_phy(\n",
                "    oe_folder / phy_folder_name, exclude_cluster_groups=cluster_group_interest\n",
                ")\n",
                "unit_labels = sorting_spikes.get_property(\"quality\")\n",
                "recording_saved = get_preprocessed_recording(oe_folder,analysis_methods)\n",
                "sorting_analyzer = si.create_sorting_analyzer(\n",
                "    sorting=sorting_spikes,\n",
                "    recording=recording_saved,\n",
                "    sparse=True,  # default\n",
                "    format=\"memory\",  # default\n",
                ")\n",
                "analysis_methods.update({\"load_curated_spikes\": False})\n",
                "analysis_methods.update({\"save_prepocessed_file\": False,\"load_prepocessed_file\": True,\"save_sorting_file\":False,\"load_sorting_file\":True})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# if you used format='memory' in si.create_sorting_analyzer, then you need to calculate the analyser extension again \n",
                "# so that those spike properties is saved and loaded to si-gui\n",
                "# Note: it is not possible to save curation in this memory mode, but at least you can output the curation to JSON file\n",
                "calculate_analyzer_extension(sorting_analyzer)\n",
                "run_mainwindow(sorting_analyzer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#once a curation is made and saved as JSON file, it can be loaded back to si-gui\n",
                "#note: load_curation function does not seem to be working properly, it returned this error\n",
                "# ---------------------------------------------------------------------------\n",
                "# TypeError                                 Traceback (most recent call last)\n",
                "# Cell In[69], line 3\n",
                "#       1 curation_path=Path(r\"Y:\\GN25060\\251130\\coherence\\session1\\2025-11-30_14-25-01\")/\"merged_list.json\"\n",
                "#       2 with open(curation_path) as f:\n",
                "# ----> 3     curation_dict2=load_curation(f)\n",
                "\n",
                "# File c:\\Users\\neuroPC\\anaconda3\\envs\\spike_interface\\lib\\site-packages\\spikeinterface\\curation\\curation_format.py:297, in load_curation(curation_path)\n",
                "#     283 def load_curation(curation_path: str | Path) -> CurationModel:\n",
                "#     284     \"\"\"\n",
                "#     285     Loads a curation from a local json file.\n",
                "#     286 \n",
                "#    (...)\n",
                "#     295         A CurationModel object\n",
                "#     296     \"\"\"\n",
                "# --> 297     with open(curation_path) as f:\n",
                "#     298         curation_dict = json.load(f)\n",
                "#     299     return CurationModel(**curation_dict)\n",
                "\n",
                "# TypeError: expected str, bytes or os.PathLike object, not TextIOWrapper\n",
                "\n",
                "with open(r\"Y:\\GN25060\\251130\\coherence\\session1\\2025-11-30_14-25-01\\merged_list.json\") as f:\n",
                "    curation_dict=json.load(f)\n",
                "run_mainwindow(sorting_analyzer, curation=True,curation_dict=curation_dict)\n",
                "#sorting_analyzer.save_as(folder=oe_folder/'merged_test', format=\"zarr\")\n",
                "\n",
                "# curation_path=Path(r\"Y:\\GN25060\\251130\\coherence\\session1\\2025-11-30_14-25-01\")/\"merged_list.json\"\n",
                "# with open(curation_path) as f:\n",
                "#     curation_dict2=load_curation(f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#if analyser is saved in the disk, it can be loaded directly\n",
                "#Note: in this node, all curation records can be saved directly to the zarr file (but to apply for changes, you still need to output the data to JSON file)\n",
                "GN25060_analyzer = si.load_sorting_analyzer(r\"Y:\\GN25060\\251130\\coherence\\session1\\2025-11-30_14-25-01\\analyser_SC2.zarr\")\n",
                "run_mainwindow(GN25060_analyzer,curation=True)\n",
                "#GN25060_analyzer.save_as(folder=oe_folder/'saved_test', format=\"zarr\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "clean_sorting_analyzer = apply_curation(GN25060_analyzer,curation_dict_or_model=curation_dict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "SortingAnalyzer: 62 channels - 79 units - 1 segments - zarr - sparse - has recording\n",
                            "Loaded 13 extensions: correlograms, isi_histograms, noise_levels, random_spikes, waveforms, principal_components, templates, spike_amplitudes, spike_locations, template_metrics, template_similarity, unit_locations, quality_metrics"
                        ]
                    },
                    "execution_count": 57,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "clean_sorting_analyzer.save_as(folder=oe_folder/'analyser_merged_SC2', format=\"zarr\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Note: at the moment, a curated analyser cannot be opened in si-gui\n",
                "run_mainwindow(clean_sorting_analyzer)\n",
                "# %gui qt\n",
                "# sw.plot_sorting_summary(sorting_analyzer, backend=\"spikeinterface_gui\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 3.2: spike sorting curation and create spike analyser as a database"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def that_ephys_folder_not_exist(base_directory,pattern):\n",
                "    for item in os.listdir(base_directory):\n",
                "        item_path = os.path.join(base_directory, item)\n",
                "        if os.path.isdir(item_path) and item.startswith(pattern):\n",
                "            return False\n",
                "    return True\n",
                "for this_dir in dir_list:\n",
                "    if that_ephys_folder_not_exist(this_dir,\"phy\"):\n",
                "        print(f\"The directory '{this_dir}' does not contain any folders starting with phy. That means the manual curation process is not done\")\n",
                "        #spike_curation(this_dir,analysis_methods)\n",
                "    elif that_ephys_folder_not_exist(this_dir,\"analyser\"):\n",
                "        print(f\"The directory '{this_dir}' does not contain any folders starting with analyser. That means the curated data has not been process with anlayser yet\")\n",
                "    else:\n",
                "        print(f\"The directory '{this_dir}' have both folders. Hence it is ready to move on to the next session.\")\n",
                "        continue\n",
                "        #decode_spikes(this_dir,analysis_methods)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 3.3: Sync ephys data with other datasets so that we can plot spike rate in response to the onset of certain events"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 4: Validate the result of automatic sorters"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Session 4.1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import spikeinterface.comparison as scom\n",
                "import spikeinterface.widgets as sw\n",
                "from spike_curation import calculate_analyzer_extension,spike_overview\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#this_dir = r\"Y:\\GN25037\\250922\\looming\\session2\\2025-09-22_15-59-41\"\n",
                "#oe_folder = Path(this_dir)\n",
                "oe_folder=r'Y:\\GN25033\\250906\\looming\\session1\\2025-09-06_18-42-24'\n",
                "json_file = \"./analysis_methods_dictionary.json\"\n",
                "\n",
                "if isinstance(json_file, dict):\n",
                "    analysis_methods = json_file\n",
                "else:\n",
                "    with open(json_file, \"r\") as f:\n",
                "        print(f\"load analysis methods from file {json_file}\")\n",
                "        analysis_methods = json.loads(f.read())\n",
                "this_experimenter = analysis_methods.get(\"experimenter\")\n",
                "if analysis_methods.get(\"include_MUA\") == True:\n",
                "    cluster_group_interest = [\"noise\"]\n",
                "else:\n",
                "    cluster_group_interest = [\"noise\", \"mua\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sorter_list=[\"kilosort4\",\"spykingcircus2\"]\n",
                "unit_list=[]\n",
                "analyser_list=[]\n",
                "for this_sorter in sorter_list:\n",
                "    sorter_suffix = generate_sorter_suffix(this_sorter)\n",
                "    phy_folder_name = \"phy\" + sorter_suffix\n",
                "    analyser_folder_name = \"analyser\" + sorter_suffix\n",
                "    analyser_list.append(analyser_folder_name)\n",
                "    unit_list.append(se.read_phy(\n",
                "        oe_folder / phy_folder_name, exclude_cluster_groups=cluster_group_interest\n",
                "    ))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "comp2sorters= scom.compare_two_sorters(unit_list[0], unit_list[1], sorter_list[0], sorter_list[1])\n",
                "w = sw.plot_agreement_matrix(comp2sorters)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "multi_comp = scom.compare_multiple_sorters(\n",
                "    sorting_list=unit_list,\n",
                "    name_list=sorter_list,\n",
                "    spiketrain_mode='union',\n",
                "    verbose=True\n",
                ")\n",
                "w = sw.plot_multicomparison_agreement(multi_comp) # k sorters means the number of sorters\n",
                "# w = sw.plot_multicomparison_agreement_by_sorter(multi_comp)\n",
                "# w = sw.plot_multicomparison_graph(multi_comp)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for sorting_spikes,analyser_folder_name in zip(unit_list,analyser_list):\n",
                "    unit_labels = sorting_spikes.get_property(\"quality\")\n",
                "    recording_saved = get_preprocessed_recording(oe_folder,analysis_methods)\n",
                "    analysis_methods.update({\"load_existing_motion_info\": True})\n",
                "    recording_saved=si.astype(recording_saved,np.float32)\n",
                "    recording_corrected_dict=motion_correction_shankbyshank(recording_saved,oe_folder,analysis_methods)\n",
                "    if len(recording_corrected_dict)>1:\n",
                "        recording_for_analysis=si.aggregate_channels(recording_corrected_dict)\n",
                "    else:\n",
                "        recording_for_analysis=recording_corrected_dict[0]\n",
                "    sorting_analyzer = si.create_sorting_analyzer(\n",
                "        sorting=sorting_spikes,\n",
                "        recording=recording_for_analysis,\n",
                "        sparse=True,  # default\n",
                "        format=\"binary_folder\",\n",
                "        folder=oe_folder / analyser_folder_name,\n",
                "        overwrite=True,  # default  # default\n",
                "    )\n",
                "    calculate_analyzer_extension(sorting_analyzer)\n",
                "    metric_names = si.get_quality_metric_list()\n",
                "    qm = si.compute_quality_metrics(sorting_analyzer, metric_names=metric_names, verbose=True)\n",
                "    display(qm)\n",
                "    _, _, _, _ = spike_overview(\n",
                "        oe_folder,\n",
                "        this_sorter,\n",
                "        sorting_spikes,\n",
                "        sorting_analyzer,\n",
                "        recording_for_analysis,\n",
                "        unit_labels,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 5: Running spike sorting with kilosort standalone"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from kilosort.run_kilosort import run_kilosort\n",
                "from pathlib import Path\n",
                "import os,json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from kilosort.io import load_ops,load_probe\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib import gridspec, rcParams"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# outputs saved to results_dir\n",
                "def load_result_kilosort(results_dir):\n",
                "    ops = load_ops(results_dir / 'ops.npy')\n",
                "    camps = pd.read_csv(results_dir / 'cluster_Amplitude.tsv', sep='\\t')['Amplitude'].values\n",
                "    contam_pct = pd.read_csv(results_dir / 'cluster_ContamPct.tsv', sep='\\t')['ContamPct'].values\n",
                "    chan_map =  np.load(results_dir / 'channel_map.npy')\n",
                "    templates =  np.load(results_dir / 'templates.npy')\n",
                "    chan_best = (templates**2).sum(axis=1).argmax(axis=-1)\n",
                "    chan_best = chan_map[chan_best]\n",
                "    amplitudes = np.load(results_dir / 'amplitudes.npy')\n",
                "    st = np.load(results_dir / 'spike_times.npy')\n",
                "    clu = np.load(results_dir / 'spike_clusters.npy')\n",
                "    firing_rates = np.unique(clu, return_counts=True)[1] * ops['fs'] / st.max()\n",
                "    dshift = ops['dshift']\n",
                "    return ops, camps,contam_pct,chan_map,templates,chan_best,amplitudes,st,clu,firing_rates,dshift"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_binary_files(base_directory, file_type):\n",
                "    selected_files = []\n",
                "    for root, _, files in os.walk(base_directory):\n",
                "        for file in files:\n",
                "            #if file.lower().endswith(file_type) and '.acquisition_board' in root:\n",
                "            if file.lower().endswith(file_type) and 'Acquisition_Board-100.acquisition_board' in root:\n",
                "                p=Path(root)\n",
                "                selected_files.append(p.joinpath(file))\n",
                "    return selected_files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_overview_kilosort(settings,dshift,chan_best,chan_map,firing_rates,camps,contam_pct,templates,st,ops,clu):\n",
                "    display_in_sec=5\n",
                "    rcParams['axes.spines.top'] = False\n",
                "    rcParams['axes.spines.right'] = False\n",
                "    gray = .5 * np.ones(3)\n",
                "\n",
                "    fig = plt.figure(figsize=(10,10), dpi=100)\n",
                "    grid = gridspec.GridSpec(3, 3, figure=fig, hspace=0.5, wspace=0.5)\n",
                "    if settings.get(\"nblocks\")>0:\n",
                "        ax = fig.add_subplot(grid[0,0])\n",
                "        ax.plot(np.arange(0, ops['Nbatches'])*2, dshift)\n",
                "        ax.set_xlabel('time (sec.)')\n",
                "        ax.set_ylabel('drift (um)')\n",
                "\n",
                "    ax = fig.add_subplot(grid[0,1:])\n",
                "    t0 = 0\n",
                "    t1 = np.nonzero(st > ops['fs']*display_in_sec)[0][0]\n",
                "    ax.scatter(st[t0:t1]/ops['fs'], chan_best[clu[t0:t1]], s=0.5, color='k', alpha=0.25)\n",
                "    ax.set_xlim([0, display_in_sec])\n",
                "    ax.set_ylim([chan_map.max(), 0])\n",
                "    ax.set_xlabel('time (sec.)')\n",
                "    ax.set_ylabel('channel')\n",
                "    ax.set_title('spikes from units')\n",
                "\n",
                "    ax = fig.add_subplot(grid[1,0])\n",
                "    nb=ax.hist(firing_rates, 20, color=gray)\n",
                "    ax.set_xlabel('firing rate (Hz)')\n",
                "    ax.set_ylabel('# of units')\n",
                "\n",
                "    ax = fig.add_subplot(grid[1,1])\n",
                "    nb=ax.hist(camps, 20, color=gray)\n",
                "    ax.set_xlabel('amplitude')\n",
                "    ax.set_ylabel('# of units')\n",
                "\n",
                "    ax = fig.add_subplot(grid[1,2])\n",
                "    nb=ax.hist(np.minimum(100, contam_pct), np.arange(0,105,5), color=gray)\n",
                "    ax.plot([10, 10], [0, nb[0].max()], 'k--')\n",
                "    ax.set_xlabel('% contamination')\n",
                "    ax.set_ylabel('# of units')\n",
                "    ax.set_title('< 10% = good units')\n",
                "\n",
                "    for k in range(2):\n",
                "        ax = fig.add_subplot(grid[2,k])\n",
                "        is_ref = contam_pct<10.\n",
                "        ax.scatter(firing_rates[~is_ref], camps[~is_ref], s=3, color='r', label='mua', alpha=0.25)\n",
                "        ax.scatter(firing_rates[is_ref], camps[is_ref], s=3, color='b', label='good', alpha=0.25)\n",
                "        ax.set_ylabel('amplitude (a.u.)')\n",
                "        ax.set_xlabel('firing rate (Hz)')\n",
                "        ax.legend()\n",
                "        if k==1:\n",
                "            ax.set_xscale('log')\n",
                "            ax.set_yscale('log')\n",
                "            ax.set_title('loglog')\n",
                "    probe = ops['probe']\n",
                "    # x and y position of probe sites\n",
                "    xc, yc = probe['xc'], probe['yc']\n",
                "    nc = 16 # number of channels to show\n",
                "    good_units = np.nonzero(contam_pct <= 0.1)[0]\n",
                "    mua_units = np.nonzero(contam_pct > 0.1)[0]\n",
                "\n",
                "    gstr = ['good', 'mua']\n",
                "    for j in range(2):\n",
                "        print(f'~~~~~~~~~~~~~~ {gstr[j]} units ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
                "        print('title = number of spikes from each unit')\n",
                "        units = good_units if j==0 else mua_units\n",
                "        fig = plt.figure(figsize=(12,3), dpi=150)\n",
                "        grid = gridspec.GridSpec(2,20, figure=fig, hspace=0.25, wspace=0.5)\n",
                "\n",
                "        for k in range(40):\n",
                "            wi = units[np.random.randint(len(units))]\n",
                "            wv = templates[wi].copy()\n",
                "            cb = chan_best[wi]\n",
                "            nsp = (clu==wi).sum()\n",
                "\n",
                "            ax = fig.add_subplot(grid[k//20, k%20])\n",
                "            n_chan = wv.shape[-1]\n",
                "            ic0 = max(0, cb-nc//2)\n",
                "            ic1 = min(n_chan, cb+nc//2)\n",
                "            wv = wv[:, ic0:ic1]\n",
                "            x0, y0 = xc[ic0:ic1], yc[ic0:ic1]\n",
                "\n",
                "            amp = 4\n",
                "            for ii, (xi,yi) in enumerate(zip(x0,y0)):\n",
                "                t = np.arange(-wv.shape[0]//2,wv.shape[0]//2,1,'float32')\n",
                "                t /= wv.shape[0] / 20\n",
                "                ax.plot(xi + t, yi + wv[:,ii]*amp, lw=0.5, color='k')\n",
                "\n",
                "            ax.set_title(f'{nsp}', fontsize='small')\n",
                "            ax.axis('off')\n",
                "        plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "##specify an animal root folder to speed up the process because there are a lot of subfolders to look for\n",
                "dir_list = []\n",
                "thisDataset=r\"Y:\\GN25068\"\n",
                "for root, dirs, files in os.walk(thisDataset):\n",
                "    for folder in dirs:\n",
                "        if folder.startswith(\"Record\"):\n",
                "            dir_list.append(Path(root))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "load analysis methods from file ./analysis_methods_dictionary.json\n",
                        "load kilosort methods from file ./p2_parameters_kilosortGUI.json\n"
                    ]
                }
            ],
            "source": [
                "json_file = \"./analysis_methods_dictionary.json\"\n",
                "if isinstance(json_file, dict):\n",
                "    analysis_methods = json_file\n",
                "else:\n",
                "    with open(json_file, \"r\") as f:\n",
                "        print(f\"load analysis methods from file {json_file}\")\n",
                "        analysis_methods = json.loads(f.read())\n",
                "if analysis_methods.get(\"probe_type\")=='P2':\n",
                "    kilosort_setting_file = \"./p2_parameters_kilosortGUI.json\"\n",
                "    probe_file='./P2_RHD2132_openEphys_mapping.prb'\n",
                "elif analysis_methods.get(\"probe_type\")=='H6D':\n",
                "    kilosort_setting_file = \"./H6D_parameters_kilosortGUI.json\"\n",
                "    probe_file='./H6D_RHD2164_openEphys_mapping.prb'\n",
                "elif analysis_methods.get(\"probe_type\")==\"H10_rev\":\n",
                "    kilosort_setting_file = \"./H10_parameters_kilosortGUI_shank01.json\"\n",
                "    probe_file='./H10_RHD2164_rev_openEphys_mapping.prb'\n",
                "elif analysis_methods.get(\"probe_type\")==\"H10\":\n",
                "    kilosort_setting_file = \"./H10_parameters_kilosortGUI_shank01.json\"\n",
                "    probe_file='./H10_RHD2164_openEphys_mapping.prb'\n",
                "\n",
                "if isinstance(kilosort_setting_file, dict):\n",
                "    kilosort_methods = kilosort_setting_file\n",
                "else:\n",
                "    with open(kilosort_setting_file, \"r\") as f:\n",
                "        print(f\"load kilosort methods from file {kilosort_setting_file}\")\n",
                "        kilosort_methods = json.loads(f.read())\n",
                "settings = {**kilosort_methods['main'], **kilosort_methods['extra']}\n",
                "settings['probe_path']=probe_file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def batch_kilosort_process(settings,analysis_methods,dir_list):\n",
                "    file_type=\".dat\"\n",
                "    probe_path=settings.get('probe_path')\n",
                "    if int(settings.get(\"nblocks\"))==0:\n",
                "        ks_folder_name='kilosort4'\n",
                "    else:\n",
                "        ks_folder_name='kilosort4_motion_corrected'\n",
                "    for this_dir in dir_list:\n",
                "        if type(this_dir)==str:\n",
                "            this_dir=Path(this_dir)\n",
                "        if this_dir.stem =='2025-08-03_21-24-13':\n",
                "            these_bad_channels=[4,5,6,9]#[2,\n",
                "        elif this_dir.stem =='2025-12-05_15-29-09' or this_dir.stem =='2025-12-05_16-01-35':\n",
                "            these_bad_channels=[48]   \n",
                "        elif this_dir.stem =='2025-09-08_01-12-18' or this_dir.stem =='2025-09-08_00-40-55':\n",
                "            these_bad_channels=[23]\n",
                "        elif this_dir.stem =='2025-09-24_18-40-05':\n",
                "            these_bad_channels=[2]\n",
                "        else:\n",
                "            these_bad_channels=[]\n",
                "        selected_files=find_binary_files(this_dir, file_type)\n",
                "        if len(selected_files)>1:\n",
                "            print(\"more than one dat file is detected\")\n",
                "        else:\n",
                "            print(selected_files)\n",
                "            binary_file=selected_files[0]\n",
                "        if probe_path.startswith(\"./H10_RHD2164\"):\n",
                "            shank_of_interest=0\n",
                "        # elif probe_path.startswith(\"./P2_RHD2132\"):\n",
                "        #     shank_of_interest=0\n",
                "        else:\n",
                "            shank_of_interest=None        \n",
                "        results_dir = this_dir / ks_folder_name\n",
                "        phy_file=results_dir / f'shank_{shank_of_interest}' / 'phy.log'\n",
                "        if phy_file.is_file() and analysis_methods.get('overwrite_curated_dataset')==False:\n",
                "            print(f\".phy file found. not overwrite this {this_dir}\")\n",
                "            continue\n",
                "        else:\n",
                "            print(f\"analysing {this_dir}\")\n",
                "        ops, st, clu, tF, Wall, similar_templates, is_ref, est_contam_rate, kept_spikes=run_kilosort(settings=settings,filename=binary_file,results_dir=results_dir,bad_channels=these_bad_channels,shank_idx=shank_of_interest)\n",
                "        if shank_of_interest==None:\n",
                "            pass\n",
                "        else:\n",
                "            results_dir = results_dir / f'shank_{shank_of_interest}'\n",
                "        #_, camps,contam_pct,chan_map,templates,chan_best,amplitudes,_,_,firing_rates,dshift=load_result_kilosort(results_dir)\n",
                "        #plot_overview_kilosort(settings,dshift,chan_best,chan_map,firing_rates,camps,contam_pct,templates,st,ops,clu)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for this_dir in dir_list:\n",
                "    print(this_dir.as_posix())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_kilosort_process(settings,analysis_methods,dir_list)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "kilosort4",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
