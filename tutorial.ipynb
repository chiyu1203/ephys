{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 0.0: import dependancies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## import libraries for the analysis\n",
                "import os,sys,json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "##import modules from other folders\n",
                "current_working_directory = Path.cwd()\n",
                "parent_dir = current_working_directory.resolve().parents[0]\n",
                "sys.path.insert(0, str(parent_dir) + \"\\\\utilities\")\n",
                "from useful_tools import select_animals_gpt\n",
                "from data_cleaning import preprocess_fictrac_data\n",
                "sys.path.insert(0, str(parent_dir) + \"\\\\bonfic\")\n",
                "from analyse_stimulus_evoked_response import main"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 0.1: Load analysis methods in python dictionary form"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "json_file = \"./analysis_methods_dictionary.json\"\n",
                "with open(json_file, \"r\") as f:\n",
                "    analysis_methods = json.loads(f.read())\n",
                "    \n",
                "sheet_name=\"Zball\"\n",
                "Datasets=\"Z:/DATA/experiment_trackball_Optomotor\"\n",
                "thisDataset = f\"{Datasets}/{sheet_name}\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 0.2: check methods to use and whether some methods should be updated"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 0.3: Load animal directory as a list"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## this cell searches for a folder with a specified experiment_name under the dataset path and list up all the csv file in that folder.\n",
                "## In this project, we usually have one csv file in that folder so there is no confusion\n",
                "dir_list = []\n",
                "file_type=\".dat\"\n",
                "for root, dirs, files in os.walk(thisDataset):\n",
                "    if analysis_methods.get(\"experiment_name\") in root.split(\n",
                "        os.path.sep\n",
                "    ):  ## add this condition to avoid data from other experiments\n",
                "        for folder in dirs:\n",
                "            if folder.startswith(\"session\"):\n",
                "                folder_path=os.path.join(root,folder)\n",
                "                if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
                "                    dir_list.append(folder_path.replace(\"\\\\\", \"/\"))\n",
                "\n",
                "\n",
                "print(f\"these directories are found {dir_list}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_session_folders(base_directory, file_type, paradigm_name):\n",
                "    session_folders = []\n",
                "\n",
                "    for root, dirs, files in os.walk(base_directory):\n",
                "        # Check if the target folder (e.g., 'apple') is in the root path and the paradigm name is in the root path\n",
                "        if paradigm_name in root.split(os.path.sep):\n",
                "            for folder in dirs:\n",
                "                # Check if the folder name starts with 'session'\n",
                "                if folder.startswith(\"session\"):\n",
                "                    folder_path = os.path.join(root, folder)\n",
                "                    # Check if the folder contains at least one file with the specified file type\n",
                "                    if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
                "                        session_folders.append(folder_path)\n",
                "\n",
                "    return session_folders\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "base_directory = thisDataset\n",
                "file_type = \".dat\"\n",
                "paradigm_name = analysis_methods.get(\"experiment_name\")\n",
                "\n",
                "session_folders = find_session_folders(base_directory, file_type, paradigm_name)\n",
                "\n",
                "print(f\"These directories are found: {session_folders}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 1.0: Create fictrac curated dataset based on the list of directories"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# because the import does not update the new version of python.\n",
                "# Need to restart kernel \n",
                "for this_dir in dir_list:\n",
                "    if \"database_curated.pickle\" in os.listdir(this_dir):\n",
                "        print(f\"curated fictrac data found in {this_dir}. Skip this file\")\n",
                "        continue\n",
                "    else:\n",
                "        print(f\"no curated fictrac data in {this_dir}. Create curated file\")\n",
                "        preprocess_fictrac_data(this_dir,analysis_methods)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 1.5: load particular animals into directory list for further analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the path to your Excel file\n",
                "dir_list = []\n",
                "file_type=\".pickle\"\n",
                "using_google_sheet=True\n",
                "sheet_name = \"VCCball\"\n",
                "experiment_name=analysis_methods.get(\"experiment_name\")\n",
                "if analysis_methods.get(\"load_experiment_condition_from_database\") == True:\n",
                "    if using_google_sheet==True:\n",
                "        database_id = \"1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8\"\n",
                "                #https://docs.google.com/spreadsheets/d/1UL4eEUrQMapx9xz11-IyOSlPBcep1I9vBJ2uGgVudb8/edit?usp=sharing\n",
                "        url = f\"https://docs.google.com/spreadsheets/d/{database_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
                "        #df = pd.read_excel(url, engine='openpyxl')## use this function if the file is not google sheet but uploaded excel file\n",
                "\n",
                "        df = pd.read_csv(url)\n",
                "    else:\n",
                "        excel_file_path = \"Z:/DATA/experiment_trackball_Optomotor/Locusts Management.xlsx\"\n",
                "        print(f\"using a database {excel_file_path} from the server but this file might be outdated\")\n",
                "        # Create a 'with' statement to open and read the Excel file\n",
                "        with pd.ExcelFile(excel_file_path) as xls:\n",
                "            # Read the Excel sheet into a DataFrame with the sheet name (folder name)\n",
                "            df = pd.read_excel(xls, sheet_name)\n",
                "        ##list up the conditions and answers as strings for input argument to select animal. One condition must pair with one answer\n",
                "    if analysis_methods.get(\"select_animals_by_condition\") == True:\n",
                "        animal_of_interest=select_animals_gpt(df,\"Experimenter\",\"NS\")\n",
                "        #print(animal_of_interest)\n",
                "    else:\n",
                "        animal_of_interest=df\n",
                "    ID_array=animal_of_interest[\"ID\"].values\n",
                "    dir_list = [\n",
                "    root.replace(\"\\\\\", \"/\")\n",
                "    for root, dirs, files in os.walk(thisDataset)\n",
                "    if any(ID in root for ID in ID_array)\n",
                "    and experiment_name in root.split(os.path.sep)\n",
                "    and any(name.endswith(file_type) for name in files)\n",
                "\n",
                "\n",
                "    \n",
                "]\n",
                "else:\n",
                "    ## this cell searches for a folder with a specified experiment_name under the dataset path and list up all the hdf5 file in that folder.\n",
                "    ## However,some changes need to be made once we do sleap or deeplabcut where there are more than one H5 file generated\n",
                "    for root, dirs, files in os.walk(thisDataset):\n",
                "        if analysis_methods.get(\"experiment_name\") in root.split(os.path.sep):## add this condition to avoid data from other experiments\n",
                "            for folder in dirs:\n",
                "                if folder.startswith(\"session\"):\n",
                "                    folder_path = os.path.join(root, folder)\n",
                "                    # Check if the folder contains at least one file with the specified file type\n",
                "                    if any(name.endswith(file_type) for name in os.listdir(folder_path)):\n",
                "                        session_folders.append(folder_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 2.1: analyse individual animal's optomotor response with curated fictrac tracking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# because the import does not update the new version of python.\n",
                "# Need to restart kernel \n",
                "output0_across_exp=[]\n",
                "output1_across_exp=[]\n",
                "output2_across_exp=[]\n",
                "output3_across_exp=[]\n",
                "output4_across_exp=[]\n",
                "for this_dir in dir_list[23:]:\n",
                "    if \"archive\" in this_dir:\n",
                "        print(f\"skip archive folder for {this_dir}\")\n",
                "        continue\n",
                "    summary,speed,rotation,travel_distance_whole_session=main(this_dir,analysis_methods)\n",
                "    output0_across_exp.append(summary)\n",
                "    output1_across_exp.append(speed)\n",
                "    output2_across_exp.append(rotation)\n",
                "    output3_across_exp.append(travel_distance_whole_session)\n",
                "    output4_across_exp.append(this_dir)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 2.2: Analyse individual animal's optomotor response with the multi-engines module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##this cell start the multi-engines. Make sure to run only once\n",
                "import time\n",
                "import ipyparallel as ipp\n",
                "def show_clusters():\n",
                "    clusters = ipp.ClusterManager().load_clusters() \n",
                "    print(\"{:15} {:^10} {}\".format(\"cluster_id\", \"state\", \"cluster_file\")) \n",
                "    for c in clusters:\n",
                "        cd = clusters[c].to_dict()\n",
                "        cluster_id = cd['cluster']['cluster_id']\n",
                "        controller_state = cd['controller']['state']['state']\n",
                "        cluster_file = getattr(clusters[c], '_trait_values')['cluster_file']\n",
                "        print(\"{:15} {:^10} {}\".format(cluster_id, controller_state, cluster_file))\n",
                "    return cluster_id\n",
                "\n",
                "cluster = ipp.Cluster(n=6)\n",
                "await cluster.start_cluster()\n",
                "cluster_neuropc=show_clusters()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##input cluster_id from previous cell\n",
                "rc = ipp.Client(cluster_id=cluster_neuropc)\n",
                "\n",
                "# Create a DirectView for parallel execution\n",
                "dview = rc.direct_view()\n",
                "\n",
                "# Define a function for parallel processing\n",
                "def process_directory(this_dir, analysis_methods):\n",
                "    from analyse_stimulus_evoked_response import main\n",
                "    # Check if the H5 file (curated dataset) exists\n",
                "    summary,speed,rotation = main(this_dir, analysis_methods)\n",
                "    return (summary,speed,rotation)\n",
                "\n",
                "# Define analysis_methods\n",
                "\n",
                "# Use parallel execution to process directories\n",
                "results = dview.map_sync(process_directory, dir_list, [analysis_methods] * len(dir_list))\n",
                "\n",
                "# Initialize result lists\n",
                "output0_across_exp=[]\n",
                "output1_across_exp=[]\n",
                "output2_across_exp=[]\n",
                "\n",
                "# Collect and organize results\n",
                "for result in results:\n",
                "    if result is not None:\n",
                "        summary,speed,rotation = result\n",
                "        output0_across_exp.append(summary)\n",
                "        output1_across_exp.append(speed)\n",
                "        output2_across_exp.append(rotation)\n",
                "\n",
                "# Now, you have the results collected in the respective lists"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rc.shutdown()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 2.3: plot average behavioural data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "visual_paradigm_name= analysis_methods.get(\"experiment_name\")\n",
                "colormap = np.array(analysis_methods.get(\"graph_colour_code\"))\n",
                "fig2, (ax3, ax4) = plt.subplots(\n",
                "    nrows=1, ncols=2, figsize=(18, 7), tight_layout=True\n",
                ")\n",
                "for i in range(len(output0_across_exp)):\n",
                "    this_animal = output0_across_exp[i]\n",
                "    tmp=this_animal.groupby(\"stim_type\").count()\n",
                "    follow_count_coherence = tmp.index.values\n",
                "    for j in range(len(this_animal.groupby(\"stim_type\"))):\n",
                "        this_coherence=follow_count_coherence[j]\n",
                "        this_response = this_animal.loc[\n",
                "            this_animal[\"stim_type\"] == this_coherence, \"opto_index\"\n",
                "        ].values\n",
                "        # this_coherence = x_axis_value_running_trials[i]\n",
                "        mean_response = np.mean(this_response, axis=0)\n",
                "        sem_response = np.std(this_response, axis=0, ddof=1) / np.sqrt(\n",
                "            this_response.shape[0]\n",
                "        )\n",
                "        ax3.errorbar(\n",
                "            this_coherence,\n",
                "            mean_response,\n",
                "            yerr=sem_response,\n",
                "            c=colormap[5],\n",
                "            fmt=\"o\",\n",
                "            elinewidth=2,\n",
                "            capsize=3,\n",
                "        )\n",
                "    ax3.set_ylim(-1, 1)\n",
                "    ax3.set(\n",
                "        yticks=[-1, 0, 1],\n",
                "        ylabel=\"Optomotor Index\",\n",
                "        xlabel=visual_paradigm_name,)\n",
                "    # ax4.scatter(follow_count_coherence, follow_count, c=colormap[0], marker=\"o\")\n",
                "    # ax4.set_ylim(0, 15)\n",
                "    # ax4.set(\n",
                "    #     yticks=[0, 15],\n",
                "    #     ylabel=\"Follow response (count)\",\n",
                "    #     xticks=[100, 50, 0, -50, -100],\n",
                "    #     xlabel=\"Coherence level (%)\",\n",
                "    # )\n",
                "    ##following one dot (dot lifetime)\n",
                "    ##memory part (30s)\n",
                "    ##interval: rondot\n",
                "    ##continous"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 3: load ephys data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Session 3.0: import packages for analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%reload_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import time, os, json, warnings\n",
                "#import spikeinterface.full as si\n",
                "import spikeinterface.core as si\n",
                "import spikeinterface.extractors as se\n",
                "import spikeinterface.preprocessing as spre\n",
                "import spikeinterface.postprocessing as spost\n",
                "import spikeinterface.sorters as ss\n",
                "import spikeinterface.qualitymetrics as sq\n",
                "import spikeinterface.exporters as sep\n",
                "from spikeinterface.curation import load_curation, apply_curation\n",
                "from raw2si import *\n",
                "from spike_curation import *\n",
                "from spikeinterface_gui import run_mainwindow"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 3.1: create pre-processed dataset and apply an automatic sorter to ephys data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##specify an animal root folder to speed up the process because there are a lot of subfolders to look for\n",
                "dir_list = []\n",
                "thisDataset=r\"Y:\\GN25070\"\n",
                "for root, dirs, files in os.walk(thisDataset):\n",
                "    for folder in dirs:\n",
                "        if folder.startswith(\"Record\"):\n",
                "            dir_list.append(Path(root))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "json_file = \"./analysis_methods_dictionary.json\"\n",
                "\n",
                "if isinstance(json_file, dict):\n",
                "    analysis_methods = json_file\n",
                "else:\n",
                "    with open(json_file, \"r\") as f:\n",
                "        print(f\"load analysis methods from file {json_file}\")\n",
                "        analysis_methods = json.loads(f.read())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#analysis_methods.update({\"save_prepocessed_file\": True,\"load_prepocessed_file\": False,\"save_sorting_file\":True,\"load_sorting_file\":False,\"remove_dead_channels\":False,\"analyse_good_channels_only\":False})\n",
                "for oe_folder in dir_list:\n",
                "    if type(oe_folder)==str:\n",
                "        oe_folder=Path(oe_folder)\n",
                "    print(f\"processing {oe_folder}\")\n",
                "    raw2si(oe_folder, analysis_methods)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "phy_file_pattern=\"params*\"\n",
                "#it takes 5 hours to do postprocessing on \\GN25060\\251130\\coherence\\session1\\2025-11-30_14-25-01\\\n",
                "overwrite_existing_phy=analysis_methods.get(\"overwrite_existing_phy\")\n",
                "#analysis_methods.update({\"save_prepocessed_file\": False,\"load_prepocessed_file\": True,\"save_sorting_file\":False,\"load_sorting_file\":True})\n",
                "for oe_folder in dir_list:\n",
                "    if any(Path(oe_folder).glob(phy_file_pattern)) and overwrite_existing_phy==False:\n",
                "        continue\n",
                "    else:\n",
                "        sorting_analyzer=si2phy(oe_folder, analysis_methods)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Session 3.2: analyse single file with multiple sorters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#oe_folder=r'Y:\\GN25037\\250922\\looming\\session2\\2025-09-22_15-59-41'\n",
                "#oe_folder=r'Y:\\GN25033\\250906\\looming\\session1\\2025-09-06_18-42-24'\n",
                "#dir_list=[r'Y:\\GN25044\\251012\\looming\\session1\\2025-10-12_14-22-01',r'Y:\\GN25045\\251013\\looming\\session1\\2025-10-13_11-16-41',r'Y:\\GN25046\\251018\\looming\\session1\\2025-10-18_16-34-27',r'Y:\\GN25048\\251019\\looming\\session1\\2025-10-19_18-50-34',r'Y:\\GN25033\\250906\\looming\\session1\\2025-09-06_18-42-24']\n",
                "sorter_list=[\"spykingcircus2\",\"tridesclous2\",\"mountainsort5\",\"herdingspikes\"]\n",
                "for oe_folder in [dir_list[2]]:\n",
                "    for this_sorter in sorter_list:\n",
                "        analysis_methods.update({'sorter_name': this_sorter})\n",
                "        raw2si(oe_folder, analysis_methods)\n",
                "        _=si2phy(oe_folder, analysis_methods)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 4: visualise the result"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 4.0: visualise the result with spikeinterface-gui and add manual labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sorter_list=[\"spykingcircus2\",\"tridesclous2\",\"mountainsort5\",\"herdingspikes\",\"kilosort4\"]\n",
                "this_sorter=sorter_list[4]\n",
                "#oe_folder=r'Y:\\GN25060\\251130\\coherence\\session1\\2025-11-30_14-25-01'\n",
                "#oe_folder=r'C:\\Users\\neuroLaptop\\Documents\\GN25060\\coherence\\session1\\2025-11-30_14-25-01'\n",
                "#oe_folder=r'C:\\Users\\neuroLaptop\\Documents\\GN25070\\2025-12-28_13-34-35'\n",
                "oe_folder=dir_list[2]\n",
                "if type(oe_folder)==str:\n",
                "    oe_folder=Path(oe_folder)\n",
                "\n",
                "if analysis_methods.get(\"include_MUA\") == True:\n",
                "    cluster_group_interest = [\"noise\"]\n",
                "else:\n",
                "    cluster_group_interest = [\"noise\", \"mua\"]\n",
                "sorter_suffix = generate_sorter_suffix(this_sorter)\n",
                "phy_folder_name = \"phy\" + sorter_suffix\n",
                "analyser_folder_name = \"analyser\" + sorter_suffix + \".zarr\"\n",
                "sorting_folder_name = \"sorting\" + sorter_suffix\n",
                "if analysis_methods.get(\"load_analyser_from_disc\")==True:\n",
                "    sorting_analyzer = si.load_sorting_analyzer(oe_folder / analyser_folder_name)\n",
                "else:\n",
                "    print(\"create analyser from scratch\")\n",
                "    if analysis_methods.get(\"load_curated_spikes\")==True:\n",
                "        sorting_spikes = se.read_phy(\n",
                "        oe_folder / phy_folder_name, exclude_cluster_groups=cluster_group_interest\n",
                "    )\n",
                "        unit_labels = sorting_spikes.get_property(\"quality\")\n",
                "    elif analysis_methods.get(\"load_sorting_file\")==True:\n",
                "        sorting_spikes=si.load_extractor(oe_folder / sorting_folder_name)\n",
                "    else:\n",
                "        print(\"please specify whether to use postprocessed information based on spikeinterfae or phy. Loading_curated_spikes means read the data from phy folder, which usually comes from kilosort standalone programme\")\n",
                "    recording_saved = get_preprocessed_recording(oe_folder,analysis_methods)\n",
                "    sorting_analyzer = si.create_sorting_analyzer(\n",
                "        sorting=sorting_spikes,\n",
                "        recording=recording_saved,\n",
                "        sparse=True,  # default\n",
                "        format=\"memory\",  # default\n",
                "    )\n",
                "    calculate_analyzer_extension(sorting_analyzer) # NOTE: it is not possible to save curation in this memory mode, but at least you can output the curation to JSON file\n",
                "# NOTE: it is recommended not to save analyser and just export the labels as a json file, and then try to plot the spike data. Only overwrite the original sorting result unless you are satisfied with the curation\n",
                "run_mainwindow(sorting_analyzer,curation=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "'''\n",
                "---------------------------------------------------------------------------\n",
                "Exception                                 Traceback (most recent call last)\n",
                "Cell In[9], line 4\n",
                "      2 with open(oe_folder / curation_file_name) as f:\n",
                "      3     curation_dict=json.load(f)\n",
                "----> 4 sorting_analyzer_merged = apply_curation(sorting_analyzer,curation_dict_or_model=curation_dict)\n",
                "\n",
                "File c:\\Users\\neuroLaptop\\anaconda3\\envs\\spikeinterface\\lib\\site-packages\\spikeinterface\\curation\\curation_format.py:236, in apply_curation(sorting_or_analyzer, curation_dict_or_model, censor_ms, new_id_strategy, merging_mode, sparsity_overlap, raise_error_if_overlap_fails, verbose, **job_kwargs)\n",
                "    228         curated_sorting_or_analyzer, _, _ = apply_merges_to_sorting(\n",
                "    229             curated_sorting_or_analyzer,\n",
                "    230             merge_unit_groups=merge_unit_groups,\n",
                "   (...)\n",
                "    233             return_extra=True,\n",
                "    234         )\n",
                "    235     else:\n",
                "--> 236         curated_sorting_or_analyzer, _ = curated_sorting_or_analyzer.merge_units(\n",
                "    237             merge_unit_groups=merge_unit_groups,\n",
                "    238             censor_ms=censor_ms,\n",
                "    239             merging_mode=merging_mode,\n",
                "    240             sparsity_overlap=sparsity_overlap,\n",
                "    241             raise_error_if_overlap_fails=raise_error_if_overlap_fails,\n",
                "    242             new_id_strategy=new_id_strategy,\n",
                "    243             return_new_unit_ids=True,\n",
                "    244             format=\"memory\",\n",
                "    245             verbose=verbose,\n",
                "...\n",
                "-> 1354     raise Exception(error_message)\n",
                "   1355 else:\n",
                "   1356     warning_message = f\"The sparsity of the units in the merge groups {unmergeable_unit_groups} do not overlap enough for a soft merge using a sparsity threshold of {sparsity_overlap}. They will not be merged.\"\n",
                "\n",
                "Exception: The sparsity of the units in the merge groups [[32, 16, 17, 18, 56, 25, 26, 27, 125], [67, 5]] do not overlap enough for a soft merge using a sparsity threshold of 0.75. Either lower your `sparsity_overlap` or use the flag `raise_error_if_overlap_fails = False` to skip these units in your merge.\n",
                "'''"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Feature Request: load and apply curation with only the sorting analyser\n",
                "It seems to me that at the moment, `apply_curation` requires the argument: **curation_dict_or_model**, which is created by `load_curation` or functions that read the json file of manual curation. Since it is now possible to directly save labels and merge/split lists in the sorting analyser on spikeinterface-gui, would it be possible to use the sorting analyser for `loading_curation` (to create the curation model) and/or `apply_curation`, if the analyser has saved curation information (so without using an additional dict or curation model in the future)?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "curation_file_name=\"manual_curation_1st.json\"\n",
                "curation_dict=load_curation(oe_folder / sorting_folder_name/ curation_file_name)\n",
                "# with open(oe_folder / sorting_folder_name/ curation_file_name) as f:\n",
                "#     curation_dict=json.load(f)\n",
                "sorting_analyzer_merged = apply_curation(sorting_analyzer,curation_dict_or_model=curation_dict,sparsity_overlap=0.3)#0.5 was used for SC2, 0.3 was used for HS2 and MS5"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run_mainwindow(sorting_analyzer_merged,curation=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "analyser_merged_folder_name=\"analyser_merged\" + sorter_suffix\n",
                "sorting_analyzer_merged.save_as(folder=oe_folder/analyser_merged_folder_name, format=\"zarr\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#once a curation is made and saved as JSON file, it can be loaded back to si-gui\n",
                "#note: load_curation function does not seem to be working properly, it returned this error\n",
                "# ---------------------------------------------------------------------------\n",
                "# TypeError                                 Traceback (most recent call last)\n",
                "# Cell In[69], line 3\n",
                "#       1 curation_path=Path(r\"Y:\\GN25060\\251130\\coherence\\session1\\2025-11-30_14-25-01\")/\"merged_list.json\"\n",
                "#       2 with open(curation_path) as f:\n",
                "# ----> 3     curation_dict2=load_curation(f)\n",
                "\n",
                "# File c:\\Users\\neuroPC\\anaconda3\\envs\\spike_interface\\lib\\site-packages\\spikeinterface\\curation\\curation_format.py:297, in load_curation(curation_path)\n",
                "#     283 def load_curation(curation_path: str | Path) -> CurationModel:\n",
                "#     284     \"\"\"\n",
                "#     285     Loads a curation from a local json file.\n",
                "#     286 \n",
                "#    (...)\n",
                "#     295         A CurationModel object\n",
                "#     296     \"\"\"\n",
                "# --> 297     with open(curation_path) as f:\n",
                "#     298         curation_dict = json.load(f)\n",
                "#     299     return CurationModel(**curation_dict)\n",
                "\n",
                "# TypeError: expected str, bytes or os.PathLike object, not TextIOWrapper\n",
                "\n",
                "with open(r\"Y:\\GN25060\\251130\\coherence\\session1\\2025-11-30_14-25-01\\merged_list.json\") as f:\n",
                "    curation_dict=json.load(f)\n",
                "run_mainwindow(sorting_analyzer, curation=True,curation_dict=curation_dict)\n",
                "#sorting_analyzer.save_as(folder=oe_folder/'merged_test', format=\"zarr\")\n",
                "\n",
                "# curation_path=Path(r\"Y:\\GN25060\\251130\\coherence\\session1\\2025-11-30_14-25-01\")/\"merged_list.json\"\n",
                "# with open(curation_path) as f:\n",
                "#     curation_dict2=load_curation(f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %gui qt\n",
                "# sw.plot_sorting_summary(sorting_analyzer, backend=\"spikeinterface_gui\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 4.1: create a sorting analyser directly from kilosort4 standalone output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def spikeinterface_unit_classification(sorting_analyzer):\n",
                "    noise_neuron_labels = scur.auto_label_units(sorting_analyzer = sorting_analyzer,repo_id =\"SpikeInterface/UnitRefine_noise_neural_classifier\",trust_model=True) #or ['numpy.dtype']\n",
                "    noise_units = noise_neuron_labels[noise_neuron_labels['prediction']=='noise']\n",
                "    #noise_units.to_csv(oe_folder / sorting_folder_name/'predicted_noise_units.csv')\n",
                "    #print(noise_units)\n",
                "    analyzer_neural = sorting_analyzer.remove_units(noise_units.index)\n",
                "    # Apply the sua/mua model\n",
                "    sua_mua_labels = scur.auto_label_units(\n",
                "        sorting_analyzer=analyzer_neural,\n",
                "        repo_id=\"SpikeInterface/UnitRefine_sua_mua_classifier\",\n",
                "        trust_model=True,\n",
                "    )\n",
                "    all_labels = pd.concat([sua_mua_labels, noise_units]).sort_index()\n",
                "    #all_labels.to_csv(oe_folder / sorting_folder_name/'predicted_sua_mua.csv')\n",
                "    print(all_labels[all_labels['prediction']=='sua'])\n",
                "    return all_labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "oe_folder=dir_list[2]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#trying to do postprocessing without motion correctin and whitening\n",
                "recording_saved = get_preprocessed_recording(oe_folder,analysis_methods)\n",
                "kilosort_standlone_folder='kilosort4_motion_corrected'\n",
                "shank_number=0\n",
                "kilosort_standlone_subfolder = f'shank_{shank_number}'\n",
                "sorting = se.read_kilosort(oe_folder /kilosort_standlone_folder/kilosort_standlone_subfolder)\n",
                "\n",
                "sorting_analyzer = si.create_sorting_analyzer(sorting, recording_saved)\n",
                "calculate_analyzer_extension(sorting_analyzer)\n",
                "run_mainwindow(sorting_analyzer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_labels=spikeinterface_unit_classification(sorting_analyzer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# a new function that directly read the output into an analyzer https://spikeinterface.readthedocs.io/en/latest/how_to/import_kilosort_data.html this is still out of construction\n",
                "# recording_saved = get_preprocessed_recording(oe_folder,analysis_methods)\n",
                "# kilosort_standlone_folder='kilosort4_motion_corrected'\n",
                "# shank_number=0\n",
                "# kilosort_standlone_subfolder = f'shank_{shank_number}'\n",
                "sorting_analyzer_new = se.read_kilosort_as_analyzer(oe_folder /kilosort_standlone_folder/kilosort_standlone_subfolder)\n",
                "sorting_analyzer_new.set_temporary_recording(recording_saved)\n",
                "all_labels_new=spikeinterface_unit_classification(sorting_analyzer_new)\n",
                "run_mainwindow(sorting_analyzer_new)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sorting_analyzer_new.compute({\n",
                "    \"spike_locations\": {},\n",
                "    \"spike_amplitudes\": {},\n",
                "    \"unit_locations\": {},\n",
                "    \"correlograms\": {},\n",
                "    \"template_similarity\": {},\n",
                "    \"isi_histograms\": {},\n",
                "    \"template_metrics\": {\"include_multi_channel_metrics\": True},\n",
                "    \"quality_metrics\": {},\n",
                "})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#trying to do postprocessing first motion correction and then whitening\n",
                "(win_step_um,win_scale_um)=(75,150)\n",
                "motion_corrector='kilosort_like'\n",
                "motion_corrector_params = spre.get_motion_parameters_preset(motion_corrector)\n",
                "motion_corrector_params['estimate_motion_kwargs'].update({\"win_step_um\":win_step_um,\"win_scale_um\":win_scale_um})\n",
                "sorting = scur.remove_excess_spikes(sorting=sorting, recording=recording_saved)  \n",
                "sorting_analyzer_whitened_later = si.create_sorting_analyzer(sorting, spre.whiten(recording=spre.correct_motion(recording=spre.astype(recording_saved,np.float32),preset=motion_corrector,estimate_motion_kwargs=motion_corrector_params['estimate_motion_kwargs']),mode=\"local\",radius_um=150,int_scale=200))\n",
                "calculate_analyzer_extension(sorting_analyzer_whitened_later)\n",
                "run_mainwindow(sorting_analyzer_whitened_later)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_labels_whitened_later=spikeinterface_unit_classification(sorting_analyzer_whitened_later)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#trying to do postprocessing first whitening and then motion correction\n",
                "# (win_step_um,win_scale_um)=(75,150)\n",
                "# motion_corrector='kilosort_like'\n",
                "# motion_corrector_params = spre.get_motion_parameters_preset(motion_corrector)\n",
                "# motion_corrector_params['estimate_motion_kwargs'].update({\"win_step_um\":win_step_um,\"win_scale_um\":win_scale_um})\n",
                "sorting_analyzer_whitened_first = si.create_sorting_analyzer(sorting, spre.correct_motion(recording=spre.whiten(recording=spre.astype(recording_saved,np.float32),mode=\"local\",radius_um=150,int_scale=200),preset=motion_corrector,estimate_motion_kwargs=motion_corrector_params['estimate_motion_kwargs']))              \n",
                "calculate_analyzer_extension(sorting_analyzer_whitened_first)\n",
                "run_mainwindow(sorting_analyzer_whitened_first)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_labels_whitened_first=spikeinterface_unit_classification(sorting_analyzer_whitened_first)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 4.2 Training models to do automatic labeling"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### concatenate units across sessions. For at least 500 units. For more information https://spikeinterface.readthedocs.io/en/latest/tutorials/curation/plot_2_train_a_model.html#sphx-glr-tutorials-curation-plot-2-train-a-model-py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "both_sortings = si.aggregate_units([sorting_1, sorting_2])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer = sc.train_model(\n",
                "    mode = \"analyzers\", # You can supply a labelled csv file instead of an analyzer\n",
                "    labels = [labels],\n",
                "    analyzers = [analyzer],\n",
                "    folder = \"my_folder\", # Where to save the model and model_info.json file\n",
                "    metric_names = None, # Specify which metrics to use for training: by default uses those already calculted\n",
                "    imputation_strategies = [\"median\"], # Defaults to all\n",
                "    scaling_techniques = [\"standard_scaler\"], # Defaults to all\n",
                "    classifiers = None, # Default to Random Forest only. Other classifiers you can try [ \"AdaBoostClassifier\",\"GradientBoostingClassifier\",\"LogisticRegression\",\"MLPClassifier\"]\n",
                "    overwrite = True, # Whether or not to overwrite `folder` if it already exists. Default is False.\n",
                "    search_kwargs = {'cv': 3} # Parameters used during the model hyperparameter search\n",
                ")\n",
                "\n",
                "best_model = trainer.best_pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "accuracies = pd.read_csv(Path(\"my_folder\") / \"model_accuracies.csv\", index_col = 0)\n",
                "accuracies.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot feature importances\n",
                "importances = best_model.named_steps['classifier'].feature_importances_\n",
                "indices = np.argsort(importances)[::-1]\n",
                "\n",
                "# The sklearn importances are not computed for inputs whose values are all `nan`.\n",
                "# Hence, we need to pick out the non-`nan` columns of our metrics\n",
                "features = best_model.feature_names_in_\n",
                "n_features = best_model.n_features_in_\n",
                "\n",
                "metrics = pd.concat([analyzer.get_extension('quality_metrics').get_data(), analyzer.get_extension('template_metrics').get_data()], axis=1)\n",
                "non_null_metrics = ~(metrics.isnull().all()).values\n",
                "\n",
                "features = features[non_null_metrics]\n",
                "n_features = len(features)\n",
                "\n",
                "plt.figure(figsize=(12, 7))\n",
                "plt.title(\"Feature Importances\")\n",
                "plt.bar(range(n_features), importances[indices], align=\"center\")\n",
                "plt.xticks(range(n_features), features[indices], rotation=90)\n",
                "plt.xlim([-1, n_features])\n",
                "plt.subplots_adjust(bottom=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Session 4.3 Comparing result across automatic sorters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import spikeinterface.comparison as scom\n",
                "import spikeinterface.widgets as sw\n",
                "from spike_curation import calculate_analyzer_extension,spike_overview\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#this_dir = r\"Y:\\GN25037\\250922\\looming\\session2\\2025-09-22_15-59-41\"\n",
                "#oe_folder = Path(this_dir)\n",
                "oe_folder=r'Y:\\GN25033\\250906\\looming\\session1\\2025-09-06_18-42-24'\n",
                "json_file = \"./analysis_methods_dictionary.json\"\n",
                "\n",
                "if isinstance(json_file, dict):\n",
                "    analysis_methods = json_file\n",
                "else:\n",
                "    with open(json_file, \"r\") as f:\n",
                "        print(f\"load analysis methods from file {json_file}\")\n",
                "        analysis_methods = json.loads(f.read())\n",
                "this_experimenter = analysis_methods.get(\"experimenter\")\n",
                "if analysis_methods.get(\"include_MUA\") == True:\n",
                "    cluster_group_interest = [\"noise\"]\n",
                "else:\n",
                "    cluster_group_interest = [\"noise\", \"mua\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sorter_list=[\"kilosort4\",\"spykingcircus2\"]\n",
                "#sorter_list=[\"spykingcircus2\",\"tridesclous2\",\"mountainsort5\",\"herdingspikes\"]\n",
                "unit_list=[]\n",
                "analyser_list=[]\n",
                "for this_sorter in sorter_list:\n",
                "    sorter_suffix = generate_sorter_suffix(this_sorter)\n",
                "    phy_folder_name = \"phy\" + sorter_suffix\n",
                "    analyser_folder_name = \"analyser\" + sorter_suffix + \".zarr\"\n",
                "    sorting_folder_name = \"sorting\" + sorter_suffix\n",
                "    if analysis_methods.get(\"load_sorting_file\")==True or analysis_methods.get(\"load_analyser_from_disc\")==True:\n",
                "        print(f\"load sorting results from sorting folder {this_sorter}\")\n",
                "        unit_list.append(si.load_extractor(oe_folder / sorting_folder_name))\n",
                "        #unit_list.append(si.load_sorting_analyzer(oe_folder / analyser_folder_name))\n",
                "    elif analysis_methods.get(\"load_curated_spikes\")==True:\n",
                "        print(f\"load sorting results from phy folder {this_sorter}\")\n",
                "        unit_list.append(se.read_phy(\n",
                "            oe_folder / phy_folder_name, exclude_cluster_groups=cluster_group_interest\n",
                "        ))\n",
                "    else:\n",
                "        print(\"please specify whether to use postprocessed information based on spikeinterfae or phy. Loading_curated_spikes means read the data from phy folder, which usually comes from kilosort standalone programme\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "comp2sorters= scom.compare_two_sorters(unit_list[0], unit_list[1], sorter_list[0], sorter_list[1])\n",
                "sw.plot_agreement_matrix(comp2sorters)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "multi_comp = scom.compare_multiple_sorters(\n",
                "    sorting_list=unit_list,\n",
                "    name_list=sorter_list,\n",
                "    spiketrain_mode='union',\n",
                "    verbose=True\n",
                ")\n",
                "sw.plot_multicomparison_agreement(multi_comp) # k sorters means the number of sorters\n",
                "sw.plot_multicomparison_agreement_by_sorter(multi_comp)\n",
                "sw.plot_multicomparison_graph(multi_comp)\n",
                "print(multi_comp.comparisons[(sorter_list[0], sorter_list[1])].sorting1,multi_comp.comparisons[(sorter_list[0], sorter_list[1])].sorting2)\n",
                "print(multi_comp.comparisons[(sorter_list[0], sorter_list[1])].get_matching())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "agr_all=multi_comp.get_agreement_sorting()\n",
                "agr_all.get_unit_ids()\n",
                "agr_2=multi_comp.get_agreement_sorting(minimum_agreement_count=2)\n",
                "agr_2.get_unit_ids()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Session 5: Running spike sorting with kilosort standalone"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from kilosort.run_kilosort import run_kilosort\n",
                "from pathlib import Path\n",
                "import os,json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from kilosort.io import load_ops,load_probe\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib import gridspec, rcParams"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# outputs saved to results_dir\n",
                "def load_result_kilosort(results_dir):\n",
                "    ops = load_ops(results_dir / 'ops.npy')\n",
                "    camps = pd.read_csv(results_dir / 'cluster_Amplitude.tsv', sep='\\t')['Amplitude'].values\n",
                "    contam_pct = pd.read_csv(results_dir / 'cluster_ContamPct.tsv', sep='\\t')['ContamPct'].values\n",
                "    chan_map =  np.load(results_dir / 'channel_map.npy')\n",
                "    templates =  np.load(results_dir / 'templates.npy')\n",
                "    chan_best = (templates**2).sum(axis=1).argmax(axis=-1)\n",
                "    chan_best = chan_map[chan_best]\n",
                "    amplitudes = np.load(results_dir / 'amplitudes.npy')\n",
                "    st = np.load(results_dir / 'spike_times.npy')\n",
                "    clu = np.load(results_dir / 'spike_clusters.npy')\n",
                "    firing_rates = np.unique(clu, return_counts=True)[1] * ops['fs'] / st.max()\n",
                "    dshift = ops['dshift']\n",
                "    return ops, camps,contam_pct,chan_map,templates,chan_best,amplitudes,st,clu,firing_rates,dshift"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_binary_files(base_directory, file_type):\n",
                "    selected_files = []\n",
                "    for root, _, files in os.walk(base_directory):\n",
                "        for file in files:\n",
                "            #if file.lower().endswith(file_type) and '.acquisition_board' in root:\n",
                "            if file.lower().endswith(file_type) and 'Acquisition_Board-100.acquisition_board' in root:\n",
                "                p=Path(root)\n",
                "                selected_files.append(p.joinpath(file))\n",
                "    return selected_files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_overview_kilosort(settings,dshift,chan_best,chan_map,firing_rates,camps,contam_pct,templates,st,ops,clu):\n",
                "    display_in_sec=5\n",
                "    rcParams['axes.spines.top'] = False\n",
                "    rcParams['axes.spines.right'] = False\n",
                "    gray = .5 * np.ones(3)\n",
                "\n",
                "    fig = plt.figure(figsize=(10,10), dpi=100)\n",
                "    grid = gridspec.GridSpec(3, 3, figure=fig, hspace=0.5, wspace=0.5)\n",
                "    if settings.get(\"nblocks\")>0:\n",
                "        ax = fig.add_subplot(grid[0,0])\n",
                "        ax.plot(np.arange(0, ops['Nbatches'])*2, dshift)\n",
                "        ax.set_xlabel('time (sec.)')\n",
                "        ax.set_ylabel('drift (um)')\n",
                "\n",
                "    ax = fig.add_subplot(grid[0,1:])\n",
                "    t0 = 0\n",
                "    t1 = np.nonzero(st > ops['fs']*display_in_sec)[0][0]\n",
                "    ax.scatter(st[t0:t1]/ops['fs'], chan_best[clu[t0:t1]], s=0.5, color='k', alpha=0.25)\n",
                "    ax.set_xlim([0, display_in_sec])\n",
                "    ax.set_ylim([chan_map.max(), 0])\n",
                "    ax.set_xlabel('time (sec.)')\n",
                "    ax.set_ylabel('channel')\n",
                "    ax.set_title('spikes from units')\n",
                "\n",
                "    ax = fig.add_subplot(grid[1,0])\n",
                "    nb=ax.hist(firing_rates, 20, color=gray)\n",
                "    ax.set_xlabel('firing rate (Hz)')\n",
                "    ax.set_ylabel('# of units')\n",
                "\n",
                "    ax = fig.add_subplot(grid[1,1])\n",
                "    nb=ax.hist(camps, 20, color=gray)\n",
                "    ax.set_xlabel('amplitude')\n",
                "    ax.set_ylabel('# of units')\n",
                "\n",
                "    ax = fig.add_subplot(grid[1,2])\n",
                "    nb=ax.hist(np.minimum(100, contam_pct), np.arange(0,105,5), color=gray)\n",
                "    ax.plot([10, 10], [0, nb[0].max()], 'k--')\n",
                "    ax.set_xlabel('% contamination')\n",
                "    ax.set_ylabel('# of units')\n",
                "    ax.set_title('< 10% = good units')\n",
                "\n",
                "    for k in range(2):\n",
                "        ax = fig.add_subplot(grid[2,k])\n",
                "        is_ref = contam_pct<10.\n",
                "        ax.scatter(firing_rates[~is_ref], camps[~is_ref], s=3, color='r', label='mua', alpha=0.25)\n",
                "        ax.scatter(firing_rates[is_ref], camps[is_ref], s=3, color='b', label='good', alpha=0.25)\n",
                "        ax.set_ylabel('amplitude (a.u.)')\n",
                "        ax.set_xlabel('firing rate (Hz)')\n",
                "        ax.legend()\n",
                "        if k==1:\n",
                "            ax.set_xscale('log')\n",
                "            ax.set_yscale('log')\n",
                "            ax.set_title('loglog')\n",
                "    probe = ops['probe']\n",
                "    # x and y position of probe sites\n",
                "    xc, yc = probe['xc'], probe['yc']\n",
                "    nc = 16 # number of channels to show\n",
                "    good_units = np.nonzero(contam_pct <= 0.1)[0]\n",
                "    mua_units = np.nonzero(contam_pct > 0.1)[0]\n",
                "\n",
                "    gstr = ['good', 'mua']\n",
                "    for j in range(2):\n",
                "        print(f'~~~~~~~~~~~~~~ {gstr[j]} units ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
                "        print('title = number of spikes from each unit')\n",
                "        units = good_units if j==0 else mua_units\n",
                "        fig = plt.figure(figsize=(12,3), dpi=150)\n",
                "        grid = gridspec.GridSpec(2,20, figure=fig, hspace=0.25, wspace=0.5)\n",
                "\n",
                "        for k in range(40):\n",
                "            wi = units[np.random.randint(len(units))]\n",
                "            wv = templates[wi].copy()\n",
                "            cb = chan_best[wi]\n",
                "            nsp = (clu==wi).sum()\n",
                "\n",
                "            ax = fig.add_subplot(grid[k//20, k%20])\n",
                "            n_chan = wv.shape[-1]\n",
                "            ic0 = max(0, cb-nc//2)\n",
                "            ic1 = min(n_chan, cb+nc//2)\n",
                "            wv = wv[:, ic0:ic1]\n",
                "            x0, y0 = xc[ic0:ic1], yc[ic0:ic1]\n",
                "\n",
                "            amp = 4\n",
                "            for ii, (xi,yi) in enumerate(zip(x0,y0)):\n",
                "                t = np.arange(-wv.shape[0]//2,wv.shape[0]//2,1,'float32')\n",
                "                t /= wv.shape[0] / 20\n",
                "                ax.plot(xi + t, yi + wv[:,ii]*amp, lw=0.5, color='k')\n",
                "\n",
                "            ax.set_title(f'{nsp}', fontsize='small')\n",
                "            ax.axis('off')\n",
                "        plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "##specify an animal root folder to speed up the process because there are a lot of subfolders to look for\n",
                "dir_list = []\n",
                "thisDataset=r\"Y:\\GN26007\"\n",
                "for root, dirs, files in os.walk(thisDataset):\n",
                "    for folder in dirs:\n",
                "        if folder.startswith(\"Record\"):\n",
                "            dir_list.append(Path(root))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "json_file = \"./analysis_methods_dictionary.json\"\n",
                "if isinstance(json_file, dict):\n",
                "    analysis_methods = json_file\n",
                "else:\n",
                "    with open(json_file, \"r\") as f:\n",
                "        print(f\"load analysis methods from file {json_file}\")\n",
                "        analysis_methods = json.loads(f.read())\n",
                "if analysis_methods.get(\"probe_type\")=='P2':\n",
                "    kilosort_setting_file = \"./p2_parameters_kilosortGUI.json\"\n",
                "    probe_file='./P2_RHD2132_openEphys_mapping.prb'\n",
                "elif analysis_methods.get(\"probe_type\")=='H6D' or analysis_methods.get(\"probe_type\")=='H6':\n",
                "    kilosort_setting_file = \"./H6D_parameters_kilosortGUI.json\"\n",
                "    probe_file='./H6D_RHD2164_openEphys_mapping.prb'\n",
                "elif analysis_methods.get(\"probe_type\")==\"H10_rev\":\n",
                "    kilosort_setting_file = \"./H10_parameters_kilosortGUI_shank01.json\"\n",
                "    probe_file='./H10_RHD2164_rev_openEphys_mapping.prb'\n",
                "elif analysis_methods.get(\"probe_type\")==\"H10\":\n",
                "    kilosort_setting_file = \"./H10_parameters_kilosortGUI_shank01.json\"\n",
                "    probe_file='./H10_RHD2164_openEphys_mapping.prb'\n",
                "elif analysis_methods.get(\"probe_type\")==\"H10_32ch\":\n",
                "    kilosort_setting_file = \"./H10_parameters_kilosortGUI_32ch.json\"\n",
                "    probe_file='./H10_RHD2164_32channels.prb'\n",
                "\n",
                "if isinstance(kilosort_setting_file, dict):\n",
                "    kilosort_methods = kilosort_setting_file\n",
                "else:\n",
                "    with open(kilosort_setting_file, \"r\") as f:\n",
                "        print(f\"load kilosort methods from file {kilosort_setting_file}\")\n",
                "        kilosort_methods = json.loads(f.read())\n",
                "settings = {**kilosort_methods['main'], **kilosort_methods['extra']}\n",
                "settings['probe_path']=probe_file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def batch_kilosort_process(settings,analysis_methods,dir_list):\n",
                "    file_type=\".dat\"\n",
                "    probe_path=settings.get('probe_path')\n",
                "    if int(settings.get(\"nblocks\"))==0:\n",
                "        ks_folder_name='kilosort4'\n",
                "    else:\n",
                "        ks_folder_name='kilosort4_motion_corrected'\n",
                "    for this_dir in dir_list:\n",
                "        if type(this_dir)==str:\n",
                "            this_dir=Path(this_dir)\n",
                "        if this_dir.stem =='2025-08-03_21-24-13':\n",
                "            these_bad_channels=[4,5,6,9]#[2,\n",
                "        elif this_dir.stem =='2025-12-05_15-29-09' or this_dir.stem =='2025-12-05_16-01-35':\n",
                "            these_bad_channels=[48]   \n",
                "        elif this_dir.stem =='2025-09-08_01-12-18' or this_dir.stem =='2025-09-08_00-40-55':\n",
                "            these_bad_channels=[23]\n",
                "        elif this_dir.stem =='2025-09-24_18-40-05':\n",
                "            these_bad_channels=[2]\n",
                "        else:\n",
                "            these_bad_channels=[]\n",
                "        selected_files=find_binary_files(this_dir, file_type)\n",
                "        if len(selected_files)>1:\n",
                "            print(\"more than one dat file is detected\")\n",
                "        else:\n",
                "            print(selected_files)\n",
                "            binary_file=selected_files[0]\n",
                "        if probe_path.startswith(\"./H10_RHD2164\"):\n",
                "            shank_of_interest=0\n",
                "        else:\n",
                "            shank_of_interest=None        \n",
                "        results_dir = this_dir / ks_folder_name\n",
                "        phy_file=results_dir / f'shank_{shank_of_interest}' / 'phy.log'\n",
                "        if phy_file.is_file() and analysis_methods.get('overwrite_curated_dataset')==False:\n",
                "            print(f\".phy file found. not overwrite this {this_dir}\")\n",
                "            continue\n",
                "        else:\n",
                "            print(f\"analysing {this_dir}\")\n",
                "        ops, st, clu, tF, Wall, similar_templates, is_ref, est_contam_rate, kept_spikes=run_kilosort(settings=settings,filename=binary_file,results_dir=results_dir,bad_channels=these_bad_channels,shank_idx=shank_of_interest)\n",
                "        if shank_of_interest==None:\n",
                "            pass\n",
                "        else:\n",
                "            results_dir = results_dir / f'shank_{shank_of_interest}'\n",
                "        #_, camps,contam_pct,chan_map,templates,chan_best,amplitudes,_,_,firing_rates,dshift=load_result_kilosort(results_dir)\n",
                "        #plot_overview_kilosort(settings,dshift,chan_best,chan_map,firing_rates,camps,contam_pct,templates,st,ops,clu)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for this_dir in dir_list:\n",
                "    print(this_dir.as_posix())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_kilosort_process(settings,analysis_methods,dir_list)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "kilosort4",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
